x
initial.possible.glotto
non.initial.possible.glotto
non.initial.possible.glotto = l.details[l.details$possiblegrammars=="2 Not initial interrogative phrase" & !is.na(l.details$possiblegrammars) & l.details$possiblegrammars!="",]$glotto
d.wh.grammar.non.initial
non.initial.grammar.glotto
initial.grammar.glotto
#data that has question words, all columns#
d.wh = alldata[alldata$meaning.id.fixed %in% whwords,]#
#variable for analyses - only wh-words words#
d.wh.m = data.frame.to.matrix(d.wh)#
fam = tapply(l.details$langFam,l.details$glotto,head,n=1)#
area = tapply(l.details$area,l.details$glotto,head,n=1)#
names2glotto = tapply(d.wh$glotto,d.wh$Language,head,n=1)#
d.wh.glotto = names2glotto[colnames(d.wh.m)]#
families.d.wh = fam[d.wh.glotto]#
areas.d.wh = area[d.wh.glotto]#
#library(maps)#
#map()#
#points(l.details[match(d.wh.glotto ,l.details$glotto),]$longitude,l.details[match(d.wh.glotto ,l.details$glotto),]$latitude,col=2,pch=16)#
#Defining variables#
initial.glotto = l.details[l.details$InterrogativePosition=="1 Initial interrogative phrase" & !is.na(l.details$InterrogativePosition) & l.details$InterrogativePosition!="",]$glotto#
non.initial.glotto = l.details[l.details$InterrogativePosition!="1 Initial interrogative phrase" & !is.na(l.details$InterrogativePosition) & l.details$InterrogativePosition!="",]$glotto#
d.wh.initial = alldata[alldata$meaning.id.fixed %in% whwords & alldata$glotto %in% initial.glotto,]#
d.wh.initial.m = data.frame.to.matrix(d.wh.initial)#
d.wh.non.initial = alldata[alldata$meaning.id.fixed %in% whwords & alldata$glotto %in% non.initial.glotto,]#
d.wh.non.initial.m = data.frame.to.matrix(d.wh.non.initial)#
#Define variables with grammars positioning info#
initial.grammar.glotto = l.details[l.details$walsandgrammars=="1 Initial interrogative phrase" & !is.na(l.details$walsandgrammars) & l.details$walsandgrammars!="",]$glotto#
non.initial.grammar.glotto = l.details[l.details$walsandgrammars=="2 Not initial interrogative phrase" & !is.na(l.details$walsandgrammars) & l.details$walsandgrammars!="",]$glotto#
d.wh.grammar.initial = alldata[alldata$meaning.id.fixed %in% whwords & alldata$glotto %in% initial.grammar.glotto,]#
d.wh.grammar.initial.m = data.frame.to.matrix(d.wh.grammar.initial)#
d.wh.grammar.non.initial = alldata[alldata$meaning.id.fixed %in% whwords & alldata$glotto %in% non.initial.grammar.glotto,]#
d.wh.grammar.non.initial.m = data.frame.to.matrix(d.wh.grammar.non.initial)#
#Define variables with possible positioning info#
initial.possible.glotto = l.details[l.details$possiblegrammars=="1 Initial interrogative phrase" & !is.na(l.details$possiblegrammars) & l.details$possiblegrammars!="",]$glotto#
non.initial.possible.glotto = l.details[l.details$possiblegrammars=="2 Not initial interrogative phrase" & !is.na(l.details$possiblegrammars) & l.details$possiblegrammars!="",]$glotto#
d.wh.possible.initial = alldata[alldata$meaning.id.fixed %in% whwords & alldata$glotto %in% initial.possible.glotto,]#
d.wh.possible.initial.m = data.frame.to.matrix(d.wh.possible.initial)#
d.wh.possible.non.initial = alldata[alldata$meaning.id.fixed %in% whwords & alldata$glotto %in% non.initial.possible.glotto,]#
d.wh.possible.non.initial.m = data.frame.to.matrix(d.wh.possible.non.initial)#
d.wh.possible.initial.glotto = names2glotto[colnames(d.wh.possible.initial.m)]#
d.wh.possible.non.initial.glotto = names2glotto[colnames(d.wh.possible.non.initial.m)]#
#
families.d.wh.possible.initial = fam[d.wh.possible.initial.glotto]#
areas.d.wh.possible.initial = area[d.wh.possible.initial.glotto]#
#
families.d.wh.possible.non.initial = fam[d.wh.possible.non.initial.glotto]#
areas.d.wh.possible.non.initial = area[d.wh.possible.non.initial.glotto]#
# create data matrix for random words#
d.random=alldata[!alldata$meaning.id.fixed %in% whwords,]#
d.random.m=data.frame.to.matrix(d.random)#
# Random - positioning WALS#
d.random.initial = alldata[(!alldata$meaning.id.fixed %in% whwords) & alldata$glotto %in% initial.glotto,]#
d.random.initial.m = data.frame.to.matrix(d.random.initial)#
d.random.non.initial = alldata[(!alldata$meaning.id.fixed %in% whwords) & alldata$glotto %in% non.initial.glotto,]#
d.random.non.initial.m = data.frame.to.matrix(d.random.non.initial)#
# Random- positioning Grammars#
d.random.grammar.initial = alldata[(!alldata$meaning.id.fixed %in% whwords) & alldata$glotto %in% initial.grammar.glotto,]#
d.random.grammar.initial.m = data.frame.to.matrix(d.random.grammar.initial)#
d.random.grammar.non.initial = alldata[(!alldata$meaning.id.fixed %in% whwords) & alldata$glotto %in% non.initial.grammar.glotto,]#
d.random.grammar.non.initial.m = data.frame.to.matrix(d.random.grammar.non.initial)#
# Random- positioning Possible#
d.random.possible.initial = alldata[(!alldata$meaning.id.fixed %in% whwords) & alldata$glotto %in% initial.possible.glotto,]#
d.random.possible.initial.m = data.frame.to.matrix(d.random.possible.initial)#
d.random.possible.non.initial = alldata[(!alldata$meaning.id.fixed %in% whwords) & alldata$glotto %in% non.initial.possible.glotto,]#
d.random.possible.non.initial.m = data.frame.to.matrix(d.random.possible.non.initial)#
#################
# Versions with only vowels or only consonants#
# load character conversion file#
charList = read.csv("../Processing/CharacterSubstitutions/extraChars3.csv",quote="",stringsAsFactors=F)#
charList = charList[charList$Consonant!="",]#
# define list of vowels#
vowels = unique(charList[charList$Consonant=="V",]$Simple)#
vowels = vowels[nchar(vowels)>0]#
vowels = unique(c('a','e','i','o','u','y',vowels))#
# define list of consonants#
consonants = unique(charList[charList$Consonant=="C",]$Simple)#
consonants = consonants[nchar(consonants)>0]#
consonants = unique(c(letters[! letters %in% vowels], "Ê’",consonants))#
# remove consonants with regular expressions#
consRegExpr = paste("[^",paste(consonants,collapse=''),"]",sep='')#
d.wh.consonantsOnly.m = t(apply(d.wh.m,1,function(X){#
	gsub(consRegExpr,"",X)#
	}))#
# remove vowels with regular expressions#
vowRegExpr = paste("[^",paste(vowels,collapse=''),"]",sep='')#
d.wh.vowelsOnly.m = t(apply(d.wh.m,1,function(X){#
	gsub(vowRegExpr,"",X)#
	}))	#
#WALS#
  #consonants#
d.wh.initial.consonantsOnly.m = t(apply(d.wh.initial.m,1,function(X){#
  gsub(consRegExpr,"",X)#
}))#
d.wh.non.initial.consonantsOnly.m = t(apply(d.wh.non.initial.m,1,function(X){#
  gsub(consRegExpr,"",X)#
}))#
  #vowels#
d.wh.initial.vowelsOnly.m = t(apply(d.wh.initial.m,1,function(X){#
  gsub(vowRegExpr,"",X)#
}))#
d.wh.non.initial.vowelsOnly.m = t(apply(d.wh.non.initial.m,1,function(X){#
  gsub(vowRegExpr,"",X)#
}))#
#Grammar#
  #consonants#
d.wh.grammar.initial.consonantsOnly.m = t(apply(d.wh.grammar.initial.m,1,function(X){#
  gsub(consRegExpr,"",X)#
}))#
d.wh.grammar.non.initial.consonantsOnly.m = t(apply(d.wh.grammar.non.initial.m,1,function(X){#
  gsub(consRegExpr,"",X)#
}))#
  #vowels#
d.wh.grammar.initial.vowelsOnly.m = t(apply(d.wh.grammar.initial.m,1,function(X){#
  gsub(vowRegExpr,"",X)#
}))#
d.wh.grammar.non.initial.vowelsOnly.m = t(apply(d.wh.grammar.non.initial.m,1,function(X){#
  gsub(vowRegExpr,"",X)#
}))#
#Possible#
  #Consonants#
d.wh.possible.initial.consonantsOnly.m = t(apply(d.wh.possible.initial.m,1,function(X){#
  gsub(consRegExpr,"",X)#
}))#
d.wh.possible.non.initial.consonantsOnly.m = t(apply(d.wh.possible.non.initial.m,1,function(X){#
  gsub(consRegExpr,"",X)#
}))#
  #Vowels#
d.wh.possible.initial.vowelsOnly.m = t(apply(d.wh.possible.initial.m,1,function(X){#
  gsub(vowRegExpr,"",X)#
}))#
d.wh.possible.non.initial.vowelsOnly.m = t(apply(d.wh.possible.non.initial.m,1,function(X){#
  gsub(vowRegExpr,"",X)#
}))#
#########
# Make specific lists of concepts#
  #face= head(4.200), face(4.204), forehead (4.205), cheek(4.208), chin(4.209), eye(4.210), ear (4.220), nose(4.230), mouth(4.240)#
  body.concepts = as.numeric(c("4.200","4.204","4.205","4.208", "4.209", "4.210", "4.220", "4.230", "4.240"))#
  d.BodyConcepts = alldata[alldata$meaning.id.fixed %in% body.concepts,]#
  d.BodyConcepts.m = data.frame.to.matrix(d.BodyConcepts)#
d.BodyConcepts.initial = d.BodyConcepts[d.BodyConcepts$glotto %in% initial.possible.glotto,]#
d.BodyConcepts.initial.m = data.frame.to.matrix(d.BodyConcepts.initial)#
#
d.BodyConcepts.non.initial = d.BodyConcepts[d.BodyConcepts$glotto %in% non.initial.possible.glotto,]#
d.BodyConcepts.non.initial.m = data.frame.to.matrix(d.BodyConcepts.non.initial)#
d.BodyConcepts.glotto.initial = names2glotto[colnames(d.BodyConcepts.initial.m)]#
d.BodyConcepts.glotto.non.initial = names2glotto[colnames(d.BodyConcepts.non.initial.m)]#
#
families.d.BodyConcepts.initial = fam[d.BodyConcepts.glotto.initial]#
areas.d.BodyConcepts.initial = area[d.BodyConcepts.glotto.initial]#
#
families.d.BodyConcepts.non.initial = fam[d.BodyConcepts.glotto.non.initial]#
areas.d.BodyConcepts.non.initial = area[d.BodyConcepts.glotto.non.initial]#
  #basic actions = do,make ("9.110"),fold ("9.150"),work ("9.120"),break ("9.260"),pull("9.330"),press ("9.342"),wash("9.360"), pour("9.350"),build ("9.440")#
  basic.actions.concepts= as.numeric(c("9.110","9.150","9.120","9.260","9.330","9.342","9.360","9.350","9.440"))#
  d.BasicActionsConcepts = alldata[alldata$meaning.id.fixed %in% basic.actions.concepts,]#
  d.BasicActionsConcepts.m = data.frame.to.matrix(d.BasicActionsConcepts)#
d.BasicActions.initial = d.BasicActionsConcepts[d.BasicActionsConcepts$glotto %in% initial.possible.glotto,]#
d.BasicActions.initial.m = data.frame.to.matrix(d.BasicActionsConcepts.initial)#
#
d.BasicActions.non.initial = d.BasicActionsConcepts[d.BasicActionsConcepts$glotto %in% non.initial.possible.glotto,]#
d.BasicActions.non.initial.m = data.frame.to.matrix(d.BasicActionsConcepts.non.initial)#
d.BasicActions.glotto.initial = names2glotto[colnames(d.BasicActions.initial.m)]#
d.BasicActions.glotto.non.initial = names2glotto[colnames(d.BasicActions.non.initial.m)]#
#
families.d.BasicActions.initial = fam[d.BasicActions.glotto.initial]#
areas.d.BasicActions.initial = area[d.BasicActions.glotto.initial]#
#
families.d.BasicActions.non.initial = fam[d.BasicActions.glotto.non.initial]#
areas.d.BasicActions.non.initial = area[d.BasicActions.glotto.non.initial]
basic.actions.concepts= as.numeric(c("9.110","9.150","9.120","9.260","9.330","9.342","9.360","9.350","9.440"))
d.BasicActionsConcepts = alldata[alldata$meaning.id.fixed %in% basic.actions.concepts,]
d.BasicActionsConcepts.m = data.frame.to.matrix(d.BasicActionsConcepts)
d.BasicActionsConcepts.initial = d.BasicActionsConcepts[d.BasicActionsConcepts$glotto %in% initial.possible.glotto,]
d.BasicActionsConcepts.initial.m = data.frame.to.matrix(d.BasicActionsConcepts.initial)
d.BasicActions.non.initial = d.BasicActionsConcepts[d.BasicActionsConcepts$glotto %in% non.initial.possible.glotto,]
d.BasicActions.non.initial.m = data.frame.to.matrix(d.BasicActionsConcepts.non.initial)
d.BasicActionsConcepts.non.initial = d.BasicActionsConcepts[d.BasicActionsConcepts$glotto %in% non.initial.possible.glotto,]
d.BasicActionsConcepts.non.initial.m = data.frame.to.matrix(d.BasicActionsConcepts.non.initial)
d.BasicActionsConcepts.glotto.initial = names2glotto[colnames(d.BasicActionsConcepts.initial.m)]#
d.BasicActionsConcepts.glotto.non.initial = names2glotto[colnames(d.BasicActionsConcepts.non.initial.m)]
families.d.BasicActions.initial = fam[d.BasicActionsConcepts.glotto.initial]#
areas.d.BasicActions.initial = area[d.BasicActionsConcepts.glotto.initial]#
#
families.d.BasicActions.non.initial = fam[d.BasicActionsConcepts.glotto.non.initial]#
areas.d.BasicActions.non.initial = area[d.BasicActionsConcepts.glotto.non.initial]
families.d.BasicActions.initial
families.d.BasicActions.non.initial
areas.d.BasicActions.initial
areas.d.BasicActions.non.initial
resultsFile = "../Results/SimplifiedPhonology/ResultsSummary_RandomIndependentSamples_RandomConcepts.txt"
number.of.perms = 10000#
number.of.random.samples = 100# number of sets of random concept sets chosen in the comparison permutation for non-wh concepts.
runComparison.randomSample(d.BodyConcepts.initial, d.BodyConcepts.non.initial, families.d.BodyConcepts.initial, families.d.BodyConcepts.non.initial, "RIS_BodyConcepts_firstSegments_Family", T)
families.d.BodyConcepts.initial
families.d.BodyConcepts.non.initial
dim(d.BodyConcepts.initial)
families.d.BodyConcepts.initial
length(families.d.BodyConcepts.initial)
sum(d.BodyConcepts$glotto %in% initial.possible.glotto)
dim(d.BodyConcepts.initial)
dim(d.BodyConcepts.initial.m)
dim(d.BodyConcepts.non.initial.m)
runComparison.randomSample(d.BodyConcepts.initial.m, d.BodyConcepts.non.initial.m, families.d.BodyConcepts.initial, families.d.BodyConcepts.non.initial, "RIS_BodyConcepts_firstSegments_Family", T)
resultsFile = "../Results/SimplifiedPhonology/ResultsSummary.txt"
#Specifes where Text file with results goes#
resultsFile = "../Results/SimplifiedPhonology/ResultsSummary_RandomIndependentSamples.txt"#
#Removes old info#
cat("",file=resultsFile) # clear results file#
#
number.of.perms = 10000#
number.of.random.samples = 100# number of sets of random concept sets chosen in the comparison permutation for non-wh concepts.
runComparison.randomSample(d.wh.possible.initial.m,d.wh.possible.non.initial.m,families.d.wh.possible.initial,families.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_firstSegments",T)
runComparison.randomSample(d.wh.possible.initial.m,d.wh.possible.non.initial.m,areas.d.wh.possible.initial,areas.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_area_firstSegments",T)
getWordListEntropy(d.wh.possible.initial.m)
x = getWordListEntropy(d.wh.possible.initial.m)
y = getWordListEntropy(d.wh.possible.non.initial.m)
plot(x)
mena(x)
mean(x)
mean(y)
colnames(x)
colnames(d.wh.possible.initial.m)
colnames(d.wh.possible.non.initial.m)
colnames(d.wh.possible.initial.m) %in% colnames(d.wh.possible.non.initial.m)
sum(colnames(d.wh.possible.initial.m) %in% colnames(d.wh.possible.non.initial.m))
initial.possible.glotto = l.details[l.details$possiblegrammars=="1 Initial interrogative phrase" & !is.na(l.details$possiblegrammars) & l.details$possiblegrammars!="",]$glotto#
non.initial.possible.glotto = l.details[l.details$possiblegrammars=="2 Not initial interrogative phrase" & !is.na(l.details$possiblegrammars) & l.details$possiblegrammars!="",]$glotto#
d.wh.possible.initial = alldata[alldata$meaning.id.fixed %in% whwords & alldata$glotto %in% initial.possible.glotto,]#
d.wh.possible.initial.m = data.frame.to.matrix(d.wh.possible.initial)#
d.wh.possible.non.initial = alldata[alldata$meaning.id.fixed %in% whwords & alldata$glotto %in% non.initial.possible.glotto,]#
d.wh.possible.non.initial.m = data.frame.to.matrix(d.wh.possible.non.initial)#
d.wh.possible.initial.glotto = names2glotto[colnames(d.wh.possible.initial.m)]#
d.wh.possible.non.initial.glotto = names2glotto[colnames(d.wh.possible.non.initial.m)]#
#
families.d.wh.possible.initial = fam[d.wh.possible.initial.glotto]#
areas.d.wh.possible.initial = area[d.wh.possible.initial.glotto]#
#
families.d.wh.possible.non.initial = fam[d.wh.possible.non.initial.glotto]#
areas.d.wh.possible.non.initial = area[d.wh.possible.non.initial.glotto]
families.d.wh.possible.initial
runComparison.randomSample(d.wh.possible.initial.m,d.wh.possible.non.initial.m,families.d.wh.possible.initial,families.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_firstSegments",T)
# Random independent sample test#
runComparison.randomSample = function(wordList1,wordList2,strat1,strat2,permName,firstSegment=F){#
	n.lang = min(length(unique(strat1)),length(unique(strat2)))#
	# compare the means of two random independent samples#
	rand.indep.comparison = replicate(number.of.perms, #
		mean( #
		getWordListEntropy(selectRandomIndependentSample(wordList1,strat1,limit=n.lang), firstSegment)  #
		) > #
		mean(#
		 getWordListEntropy(selectRandomIndependentSample(wordList2,strat2,limit=n.lang),firstSegment)#
		 )#
	) # end of replicate#
	summaryString = paste(permName,":","Number of random samples where group 1 mean entropy > group 2 mean entropy = ",sum(rand.indep.comparison,na.rm=T), "out of",sum(!is.na(rand.indep.comparison)),'permutations\n\n')#
	cat(summaryString,file=resultsFile,append=T)#
}
runComparison.randomSample(d.wh.possible.initial.m,d.wh.possible.non.initial.m,families.d.wh.possible.initial,families.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_firstSegments",T)
runComparison.randomSample(d.wh.possible.initial.m,d.wh.possible.non.initial.m,areas.d.wh.possible.initial,areas.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_area_firstSegments",T)
getWordListEntropy(d.wh.possible.initial.m,T)
t.test(getWordListEntropy(d.wh.possible.initial.m,T),getWordListEntropy(d.wh.possible.non.initial.m,T))
mean(getWordListEntropy(d.wh.possible.initial.m,T))
mean(getWordListEntropy(d.wh.possible.non.initial.m,T))
hist(getWordListEntropy(d.wh.possible.initial.m,T))#
hist(getWordListEntropy(d.wh.possible.non.initial.m,T),add=T)
hist(getWordListEntropy(d.wh.possible.initial.m,T), col=rgb(1,0,0,0.3),border=F)
hist(getWordListEntropy(d.wh.possible.non.initial.m,T),add=T, col=rgb(0,1,0,0.3),border=F)
hist(getWordListEntropy(d.wh.possible.initial.m,T), col=rgb(1,0,0,0.3),border=F,xlim=c(0,1),breaks=20)
hist(getWordListEntropy(d.wh.possible.non.initial.m,T),add=T, col=rgb(0,1,0,0.3),border=F,xlim=c(0,1),breaks=20)
x = getWordListEntropy(d.wh.possible.non.initial.m,T)
x[x==0]
families
families.d.wh
families.d.wh[x[x==0]]
families.d.wh[names(x[x==0])]
names(x[x==0])
families.d.wh[names2glotto[names(x[x==0])]]
l.details[l.details$glotto=='hind1269',]
l.details$walsandgrammars == l.details$possiblegrammars
l.details$InterrogativePosition == l.details$possiblegrammars
families.d.wh[names(x[x==0])]
g = names2glotto[names(x[x==0])]
l.details[l.details$glotto %in% g & !is.na(l.details$glotto)]
l.details[l.details$glotto %in% g & !is.na(l.details$glotto),]
families.d.wh[names(x[x==0])]
families.d.wh[names2glotto[names(x[x==0])]]
unique(families.d.wh[names2glotto[names(x[x==0])]])
x = getWordListEntropy(d.wh.possible.initial.m,T)
x
g = names2glotto[names(x[x>0.8])]
g
l.details[l.details$glotto=='rapa1244']
l.details[l.details$glotto=='rapa1244',]
d.wh.m['Rapanui']
names(d.wh.m)
colnames(d.wh.m)
d.wh.m[colnames(d.wh.m)=="Rapa_Nui_Phonemic",]
d.wh.m[colnames(d.wh.m)=="Rapa_Nui_Phonemic"]
rownames(d.wh.m)
d.wh.m[colnames(d.wh.m)=="English"]
d.wh.m[colnames(d.wh.m)=="Dutch"]
d.wh.m[1,1:5]
d.wh.m[1,30:40]
d.wh.m[2,30:40]
d.wh.m[3,30:40]
d.wh.m[4,30:40]
d.wh.m[5,30:40]
d.wh.m[6,30:40]
d.wh.m[7,30:40]
d.wh.m[8,30:40]
d.wh.m[9,30:40]
d.wh.m[9,1:10]
rm(list=ls())#
#
library(stringi)#
try(setwd("U:/Pragmatics/Slonimska/Data/Processing/Matched_word_lists"))#
#
try(setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Processing/Matched_word_lists"))#
#
Alldata<-read.csv("Alldata.csv", stringsAsFactors=F, fileEncoding='utf-8')
head(Alldata)
x = read.csv('../CleanedAndSimplifiedData/Alldata_simple.csv')
head(x)
x[grepl("sentry",x$word.simple),]
rm(list=ls())#
try(setwd("U:/Pragmatics/Slonimska/Data/RAW_data/WOLD"))#
#
try(setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/RAW_data/WOLD"))
AllWOLDdata<-read.csv("word_processed.csv",stringsAsFactors=F,encoding='utf-8')
head(AllWOLDdata)
AllWOLDdata[grepl('sentry',AllWOLDdata$meaning),]
# see VisualiseWOLD_connections.xlsx#
#
# start from word.csv, fill in name from unit.csv, get direct loan from target_word in loan.csv#
# (may need to have seperate loan table)#
# then: use couterpart.csv to link to value.csv to get meaning name from parameter.csv#
#
setwd("/Users/pplsuser/Desktop/Stuff/EtymOnline_2013/Word loanword database/clld-wold2-dbe8bcf/data/")#
#
library(jsonlite)#
#
myFromJSON = function(X,f){#
	#X = gsub("\t",",",X)#
	#X = gsub('""','"',X)#
	#X = substr(X,2,nchar(X)-1)#
	return(jsonlite::fromJSON(X)[f])#
}#
value = read.csv("value.csv",stringsAsFactors=F)#
valueset = read.csv("valueset.csv",stringsAsFactors=F)#
meaning = read.csv('meaning.csv',stringsAsFactors=F)#
word = read.csv("word.csv",stringsAsFactors=F)#
parameter = read.csv('parameter.csv',stringsAsFactors=F)#
semanticfield = read.csv("semanticfield.csv",stringsAsFactors=F)#
language = read.csv('language.csv',stringsAsFactors=F)#
unit = read.csv('unit.csv',stringsAsFactors=F)#
loan = read.csv("loan.csv",stringsAsFactors = F)#
unitdomainelement = read.csv("unitdomainelement.csv",stringsAsFactors=F)#
unitvalue = read.csv("unitvalue.csv",stringsAsFactors=F)#
counterpart = read.csv("counterpart.csv",stringsAsFactors=F)#
word_to_unit = match(word$pk,unit$pk)#
#
# some words do not have an entry in 'counterpart', which means it fails to get meaning for these#
word$word = unit[word_to_unit,]$name#
word$language_pk = unit[word_to_unit,]$language_pk#
word$target_language = language[match(word$language_pk,language$pk),]$name#
#
word$counterpart = counterpart[match(word$pk,counterpart$word_pk),]$pk#
word$valueset_pk = value[match(word$counterpart,value$pk),]$valueset_pk#
word$parameter_pk = valueset[match(word$valueset_pk,valueset$pk),]$parameter_pk#
word$meaning = parameter[match(word$parameter_pk,parameter$pk),]$name#
word$meaning.id = parameter[match(word$parameter_pk,parameter$pk),]$id#
#
word$meaning.id= gsub("\\-",'\\.',word$meaning.id)
word[word$meaning=='sentry',]
word[word$meaning=='sentry' & !is.na(word$meaning),]
head(word)
word[word$meaning=='Sentry' & !is.na(word$meaning),]
word[word$meaning.id=='17.68' & !is.na(word$meaning.id),]
word[grepl("sentry",word$meaning) & !is.na(word$meaning),]
word[grepl("sentry",word$word) & !is.na(word$word),]
missing.meaning.ids = is.na(word$meaning.id) & !is.na(word$word) & nchar(word$word)>0 & !is.na(word$target_language)
missing.meaning.ids = word$pk=="72884" & !is.na(word$pk)
unit[match(word$pk[missing.meaning.ids], unit$pk),]$description
word$meaning[missing.meaning.ids & is.na(word$meaning)] = unit[match(word$pk[missing.meaning.ids], unit$pk),]$description
word$meaning.id[ #
		match(word[missing.meaning.ids,]$meaning, #
			  word[missing.meaning.ids,]$meaning)]
word[missing.meaning.ids,]$meaning
word[missing.meaning.ids,]$meaning
word[missing.meaning.ids,]$meaning
unit[match(word$pk[missing.meaning.ids], unit$pk),]
word[!missing.meaning.ids,]$meaning
match(word[missing.meaning.ids,]$meaning, #
			  word[!missing.meaning.ids,]$meaning)
word[missing.meaning.ids,]$meaning
head(word)
head(unit)
unit[grepl("sentry",unit$description),]
unit[grepl("sentry",unit$description),]$pk
word[word$pk %in% unit[grepl("sentry",unit$description),]$pk,]
word[word$pk %in% unit[grepl("sentry",unit$description),]$pk,]$meaning.id
table(word[word$pk %in% unit[grepl("sentry",unit$description),]$pk,]$meaning.id)
t = table(word[word$pk %in% unit[grepl(X,unit$description),]$pk,]$meaning.id)
X = "sentry"
t = table(word[word$pk %in% unit[grepl(X,unit$description),]$pk,]$meaning.id)
t[which(t==max(t))]
names(t[which(t==max(t))])
names(t[which(t==max(t))])[1]
missing.meaning.ids
sum(missing.meaning.ids)
# see VisualiseWOLD_connections.xlsx#
#
# start from word.csv, fill in name from unit.csv, get direct loan from target_word in loan.csv#
# (may need to have seperate loan table)#
# then: use couterpart.csv to link to value.csv to get meaning name from parameter.csv#
#
setwd("/Users/pplsuser/Desktop/Stuff/EtymOnline_2013/Word loanword database/clld-wold2-dbe8bcf/data/")#
#
library(jsonlite)#
#
myFromJSON = function(X,f){#
	#X = gsub("\t",",",X)#
	#X = gsub('""','"',X)#
	#X = substr(X,2,nchar(X)-1)#
	return(jsonlite::fromJSON(X)[f])#
}#
value = read.csv("value.csv",stringsAsFactors=F)#
valueset = read.csv("valueset.csv",stringsAsFactors=F)#
meaning = read.csv('meaning.csv',stringsAsFactors=F)#
word = read.csv("word.csv",stringsAsFactors=F)#
parameter = read.csv('parameter.csv',stringsAsFactors=F)#
semanticfield = read.csv("semanticfield.csv",stringsAsFactors=F)#
language = read.csv('language.csv',stringsAsFactors=F)#
unit = read.csv('unit.csv',stringsAsFactors=F)#
loan = read.csv("loan.csv",stringsAsFactors = F)#
unitdomainelement = read.csv("unitdomainelement.csv",stringsAsFactors=F)#
unitvalue = read.csv("unitvalue.csv",stringsAsFactors=F)#
counterpart = read.csv("counterpart.csv",stringsAsFactors=F)#
word_to_unit = match(word$pk,unit$pk)#
#
# some words do not have an entry in 'counterpart', which means it fails to get meaning for these#
word$word = unit[word_to_unit,]$name#
word$language_pk = unit[word_to_unit,]$language_pk#
word$target_language = language[match(word$language_pk,language$pk),]$name#
#
word$counterpart = counterpart[match(word$pk,counterpart$word_pk),]$pk#
word$valueset_pk = value[match(word$counterpart,value$pk),]$valueset_pk#
word$parameter_pk = valueset[match(word$valueset_pk,valueset$pk),]$parameter_pk#
word$meaning = parameter[match(word$parameter_pk,parameter$pk),]$name#
word$meaning.id = parameter[match(word$parameter_pk,parameter$pk),]$id#
#
word$meaning.id= gsub("\\-",'\\.',word$meaning.id)#
#
# for some languages, meaning ids are missing#
missing.meaning.ids = is.na(word$meaning.id) & !is.na(word$word) & nchar(word$word)>0 & !is.na(word$target_language)#
# set meaning to description in unit#
word$meaning[missing.meaning.ids & is.na(word$meaning)] = unit[match(word$pk[missing.meaning.ids], unit$pk),]$description#
# match meaning id by meaning string#
# ERROR HERE!#
#word[missing.meaning.ids ,]$meaning.id = #
#	word$meaning.id[ #
#		match(word[missing.meaning.ids,]$meaning, #
#			  word[!missing.meaning.ids,]$meaning)]#
#
#possibly:#
#
word[missing.meaning.ids,]$meaning.id = sapply(word[missing.meaning.ids,]$meaning, function(X){#
	t = table(word[word$pk %in% unit[grepl(X,unit$description),]$pk,]$meaning.id)#
	return(names(t[which(t==max(t))])[1])#
})
sum(missing.meaning.ids)
setwd("~/Documents/Conferences/Evolang11/Reviewers/")#
#
d = read.delim("~/Documents/Conferences/Evolang11/Reviewers/NovRev3.tab",stringsAsFactors=F)#
#
reviews.per.paper = tapply(d$Status,d$X.,function(X){sum(X %in% c("awaiting your approval","review added to EasyChair"))})#
#
sum(reviews.per.paper==0)
plot(c(48,31,26))
plot(c(48,31,26),ylim=c(48,0),lty=1)
plot(c(48,31,26),ylim=c(0,48),type='l')
plot(c(48,31,26),ylim=c(0,48),type='l',xlim=c(1,10))
# see VisualiseWOLD_connections.xlsx#
#
# start from word.csv, fill in name from unit.csv, get direct loan from target_word in loan.csv#
# (may need to have seperate loan table)#
# then: use couterpart.csv to link to value.csv to get meaning name from parameter.csv#
#
setwd("/Users/pplsuser/Desktop/Stuff/EtymOnline_2013/Word loanword database/clld-wold2-dbe8bcf/data/")#
#
library(jsonlite)#
#
myFromJSON = function(X,f){#
	#X = gsub("\t",",",X)#
	#X = gsub('""','"',X)#
	#X = substr(X,2,nchar(X)-1)#
	return(jsonlite::fromJSON(X)[f])#
}#
value = read.csv("value.csv",stringsAsFactors=F)#
valueset = read.csv("valueset.csv",stringsAsFactors=F)#
meaning = read.csv('meaning.csv',stringsAsFactors=F)#
word = read.csv("word.csv",stringsAsFactors=F)#
parameter = read.csv('parameter.csv',stringsAsFactors=F)#
semanticfield = read.csv("semanticfield.csv",stringsAsFactors=F)#
language = read.csv('language.csv',stringsAsFactors=F)#
unit = read.csv('unit.csv',stringsAsFactors=F)#
loan = read.csv("loan.csv",stringsAsFactors = F)#
unitdomainelement = read.csv("unitdomainelement.csv",stringsAsFactors=F)#
unitvalue = read.csv("unitvalue.csv",stringsAsFactors=F)#
counterpart = read.csv("counterpart.csv",stringsAsFactors=F)#
word_to_unit = match(word$pk,unit$pk)#
#
# some words do not have an entry in 'counterpart', which means it fails to get meaning for these#
word$word = unit[word_to_unit,]$name#
word$language_pk = unit[word_to_unit,]$language_pk#
word$target_language = language[match(word$language_pk,language$pk),]$name#
#
word$counterpart = counterpart[match(word$pk,counterpart$word_pk),]$pk#
word$valueset_pk = value[match(word$counterpart,value$pk),]$valueset_pk#
word$parameter_pk = valueset[match(word$valueset_pk,valueset$pk),]$parameter_pk#
word$meaning = parameter[match(word$parameter_pk,parameter$pk),]$name#
word$meaning.id = parameter[match(word$parameter_pk,parameter$pk),]$id#
#
word$meaning.id= gsub("\\-",'\\.',word$meaning.id)#
#
# for some languages, meaning ids are missing#
missing.meaning.ids = is.na(word$meaning.id) & !is.na(word$word) & nchar(word$word)>0 & !is.na(word$target_language)#
# set meaning to description in unit#
word$meaning[missing.meaning.ids & is.na(word$meaning)] = unit[match(word$pk[missing.meaning.ids], unit$pk),]$description#
# match meaning id by meaning string#
# ERROR HERE!#
#word[missing.meaning.ids ,]$meaning.id = #
#	word$meaning.
sum(is.na(word$meaning))
sum(is.na(word$meaning.id))
dim(word)
wordx = word[!missing.meaning.ids & !is.na(missing.meaning.ids),c("pk","meaning.id")]
sum(is.na(word$meaning.id) & is.na(word$meaning))
head(word[is.na(word$meaning.id)])
head(word[is.na(word$meaning.id),])
word[grepl("lightness",word$meaning),]$meaning.id
sum(word[is.na(word$meaning.id),]$meaning %in% word[!is.na(word$meaning.id),]$meaning)
word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning
word[!is.na(word$meaning.id) & !is.na(word$meaning),]$meaning$id[ match(word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning,word[!is.na(word$meaning.id) & !is.na(word$meaning),]$meaning)]
word[!is.na(word$meaning.id) & !is.na(word$meaning),]$meaning.id[ match(word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning,word[!is.na(word$meaning.id) & !is.na(word$meaning),]$meaning)]
word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning[16513]
sum(is.na(word$meaning.id))
word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning.id = word[!is.na(word$meaning.id) & !is.na(word$meaning),]$meaning.id[ match(word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning,word[!is.na(word$meaning.id) & !is.na(word$meaning),]$meaning)]
sum(is.na(word$meaning.id))
head(unit)
missing.descriptions = unit[match(word[is.na(word$meaning.id) & !is.na(word$meaning),]$pk,unit$pk),]$description
head(missing.descriptions)
sapply(missing.descriptions[1:6], function(X){#
	tx = table(word[word$meaning==X,]$meaning.id)#
	return(names(tx[which(tx==max(tx))])[1])#
})
sapply(missing.descriptions[1:30], function(X){#
	tx = table(word[word$meaning==X,]$meaning.id)#
	return(names(tx[which(tx==max(tx))])[1])#
})
sapply(missing.descriptions[1:50], function(X){#
	t = table(word[word$pk %in% unit[unit$description==X),]$pk,]$meaning.id)#
	return(names(t[which(t==max(t))])[1])#
})
sapply(missing.descriptions[1:60], function(X){#
	t = table(word[word$pk %in% unit[unit$description==X,]$pk,]$meaning.id)#
	return(names(t[which(t==max(t))])[1])#
})
sum(is.na(word$meaning.id))
missing.descriptions = unit[match(word[is.na(word$meaning.id) & !is.na(word$meaning),]$pk,unit$pk),]$description#
word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning.id = sapply(missing.descriptions, function(X){#
	t = table(word[word$pk %in% unit[unit$description==X,]$pk,]$meaning.id)#
	return(names(t[which(t==max(t))])[1])#
})#
sum(is.na(word$meaning.id))
warnings()
wordx = word[is.na(word$meaning.id) & !is.na(word$meaning),c("pk","meaning.id")]
t = table(wordx[wordx$pk %in% unit[unit$description==X,]$pk,]$meaning.id)
t
length(t)
missing.descriptions = unit[match(word[is.na(word$meaning.id) & !is.na(word$meaning),]$pk,unit$pk),]$description#
wordx = word[is.na(word$meaning.id) & !is.na(word$meaning),c("pk","meaning.id")]#
word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning.id = sapply(missing.descriptions, function(X){#
	t = table(wordx[wordx$pk %in% unit[unit$description==X,]$pk,]$meaning.id)#
	if(length(t)==0){return(NA)}#
	return(names(t[which(t==max(t))])[1])#
})#
sum(is.na(word$meaning.id))
sapply(missing.descriptions[1:10], function(X){#
	if(is.na(X)){return(NA)}#
	t = table(wordx[wordx$pk %in% unit[unit$description==X,]$pk,]$meaning.id)#
	if(length(t)==0){return(NA)}#
	return(names(t[which(t==max(t))])[1])#
})
sapply(missing.descriptions[1:30], function(X){#
	if(is.na(X)){return(NA)}#
	t = table(wordx[wordx$pk %in% unit[unit$description==X,]$pk,]$meaning.id)#
	if(length(t)==0){return(NA)}#
	return(names(t[which(t==max(t))])[1])#
})
sapply(missing.descriptions[1:60], function(X){#
	if(is.na(X)){return(NA)}#
	t = table(wordx[wordx$pk %in% unit[unit$description==X,]$pk,]$meaning.id)#
	if(length(t)==0){return(NA)}#
	return(names(t[which(t==max(t))])[1])#
})
head(missing.descriptions)
X = "goat"
table(wordx[wordx$pk %in% unit[unit$description==X,]$pk,]$meaning.id)
dim(wordx)
unit[unit$description==X,]$pk
wordx$pk %in% unit[unit$description==X,]$pk
sum(wordx$pk %in% unit[unit$description==X,]$pk)
wordx[wordx$pk %in% unit[unit$description==X,]$pk,]$meaning.id
head(wordx)
wordx = word[!is.na(word$meaning.id) & !is.na(word$meaning),c("pk","meaning.id")]
wordx[wordx$pk %in% unit[unit$description==X,]$pk,]$meaning.id
wordx = word[!is.na(word$meaning.id) & !is.na(word$meaning),c("pk","meaning.id")]#
word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning.id = sapply(missing.descriptions, function(X){#
	if(is.na(X)){return(NA)}#
	t = table(wordx[wordx$pk %in% unit[unit$description==X,]$pk,]$meaning.id)#
	if(length(t)==0){return(NA)}#
	return(names(t[which(t==max(t))])[1])#
})#
sum(is.na(word$meaning.id))
missing.meanings = unit[match(word[is.na(word$meaning.id) & !is.na(word$meaning),]$pk,unit$pk),]$meaning#
wordx = word[!is.na(word$meaning.id) & !is.na(word$meaning),c("pk","meaning.id")]#
word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning.id = sapply(missing.meanings, function(X){#
	if(is.na(X)){return(NA)}#
	t = table(wordx[wordx$pk %in% unit[#
		grepl(paste("[ ,^;]",X,"[$ ,;]",sep=''),unit$description)#
		,]$pk,]$meaning.id)#
	if(length(t)==0){return(NA)}#
	return(names(t[which(t==max(t))])[1])#
})#
sum(is.na(word$meaning.id))
missing.meanings
unit[match(word[is.na(word$meaning.id) & !is.na(word$meaning),]$pk,unit$pk),]$meaning
word[is.na(word$meaning.id) & !is.na(word$meaning),]$pk
match(word[is.na(word$meaning.id) & !is.na(word$meaning),]$pk,unit$pk)
head(unit)
missing.meanings = word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning
missing.meanings
missing.meanings = word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning#
wordx = word[!is.na(word$meaning.id) & !is.na(word$meaning),c("pk","meaning.id")]#
word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning.id = sapply(missing.meanings, function(X){#
	if(is.na(X)){return(NA)}#
	t = table(wordx[wordx$pk %in% unit[#
		grepl(paste("[ ,^;]",X,"[$ ,;]",sep=''),unit$description)#
		,]$pk,]$meaning.id)#
	if(length(t)==0){return(NA)}#
	return(names(t[which(t==max(t))])[1])#
})#
sum(is.na(word$meaning.id))
missing.meanings = gsub("[\\[\\]]","",word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning)#
missing.meanings = gsub("\\*","",missing.meanings)#
missing.meanings = gsub("\\?","",missing.meanings)#
missing.meanings = gsub("\\.","",missing.meanings)#
missing.meanings = gsub("\\)","",missing.meanings)#
missing.meanings = gsub("\\(","",missing.meanings)
missing.meanings = gsub("\\+","",missing.meanings)
wordx = word[!is.na(word$meaning.id) & !is.na(word$meaning),c("pk","meaning.id")]#
word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning.id = sapply(missing.meanings, function(X){#
	if(is.na(X)){return(NA)}	#
	t = table(wordx[wordx$pk %in% unit[#
		grepl(paste("[ ,^;]",X,"[$ ,;]",sep=''),unit$description)#
		,]$pk,]$meaning.id)#
	if(length(t)==0){return(NA)}#
	return(names(t[which(t==max(t))])[1])#
})#
sum(is.na(word$meaning.id))
missing.meanings[grepl("every, each",missing.meanings)]
missing.meanings = gsub("\\[","",word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning)#
missing.meanings = gsub("\\]","",missing.meanings)#
missing.meanings = gsub("\\*","",missing.meanings)#
missing.meanings = gsub("\\?","",missing.meanings)#
missing.meanings = gsub("\\.","",missing.meanings)#
missing.meanings = gsub("\\)","",missing.meanings)#
missing.meanings = gsub("\\(","",missing.meanings)#
missing.meanings = gsub("\\+","",missing.meanings)
missing.meanings[grepl("every, each",missing.meanings)]
wordx = word[!is.na(word$meaning.id) & !is.na(word$meaning),c("pk","meaning.id")]#
word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning.id = sapply(missing.meanings, function(X){#
	if(is.na(X)){return(NA)}	#
	t = table(wordx[wordx$pk %in% unit[#
		grepl(paste("[ ,^;]",X,"[$ ,;]",sep=''),unit$description)#
		,]$pk,]$meaning.id)#
	if(length(t)==0){return(NA)}#
	return(names(t[which(t==max(t))])[1])#
})#
sum(is.na(word$meaning.id))
sapply(missing.meanings[1:30], function(X){#
	if(is.na(X)){return(NA)}	#
	t = table(wordx[wordx$pk %in% unit[#
		grepl(paste("[ ,^;]",X,"[$ ,;]",sep=''),unit$description)#
		,]$pk,]$meaning.id)#
	if(length(t)==0){return(NA)}#
	ans = names(t[which(t==max(t))])[1]#
	print(c(X,ans))#
	return(ans)#
})
word[word$meaning=='',]$meaning = NA
word[word$meaning=='' & !is.na(word$meaning),]$meaning = NA
sapply(missing.meanings[1:50], function(X){#
	if(is.na(X)){return(NA)}	#
	t = table(wordx[wordx$pk %in% unit[#
		grepl(paste("[ ,^;]",X,"[$ ,;]",sep=''),unit$description)#
		,]$pk,]$meaning.id)#
	if(length(t)==0){return(NA)}#
	ans = names(t[which(t==max(t))])[1]#
	print(c(X,ans))#
	return(ans)#
})
ls()
head(AllWOLDdata)
sum(is.na(word$meaning.id)|word$meaning.id=='')
# see VisualiseWOLD_connections.xlsx#
#
# start from word.csv, fill in name from unit.csv, get direct loan from target_word in loan.csv#
# (may need to have seperate loan table)#
# then: use couterpart.csv to link to value.csv to get meaning name from parameter.csv#
#
setwd("/Users/pplsuser/Desktop/Stuff/EtymOnline_2013/Word loanword database/clld-wold2-dbe8bcf/data/")#
#
library(jsonlite)#
#
myFromJSON = function(X,f){#
	#X = gsub("\t",",",X)#
	#X = gsub('""','"',X)#
	#X = substr(X,2,nchar(X)-1)#
	return(jsonlite::fromJSON(X)[f])#
}#
value = read.csv("value.csv",stringsAsFactors=F)#
valueset = read.csv("valueset.csv",stringsAsFactors=F)#
meaning = read.csv('meaning.csv',stringsAsFactors=F)#
word = read.csv("word.csv",stringsAsFactors=F)#
parameter = read.csv('parameter.csv',stringsAsFactors=F)#
semanticfield = read.csv("semanticfield.csv",stringsAsFactors=F)#
language = read.csv('language.csv',stringsAsFactors=F)#
unit = read.csv('unit.csv',stringsAsFactors=F)#
loan = read.csv("loan.csv",stringsAsFactors = F)#
unitdomainelement = read.csv("unitdomainelement.csv",stringsAsFactors=F)#
unitvalue = read.csv("unitvalue.csv",stringsAsFactors=F)#
counterpart = read.csv("counterpart.csv",stringsAsFactors=F)#
word_to_unit = match(word$pk,unit$pk)#
#
# some words do not have an entry in 'counterpart', which means it fails to get meaning for these#
word$word = unit[word_to_unit,]$name#
word$language_pk = unit[word_to_unit,]$language_pk#
word$target_language = language[match(word$language_pk,language$pk),]$name#
#
word$counterpart = counterpart[match(word$pk,counterpart$word_pk),]$pk#
word$valueset_pk = value[match(word$counterpart,value$pk),]$valueset_pk#
word$parameter_pk = valueset[match(word$valueset_pk,valueset$pk),]$parameter_pk#
word$meaning = parameter[match(word$parameter_pk,parameter$pk),]$name#
word$meaning.id = parameter[match(word$parameter_pk,parameter$pk),]$id#
#
word$meaning.id= gsub("\\-",'\\.',word$meaning.id)#
#
# for some languages, meaning ids are missing#
sum(is.na(word$meaning.id))#
#
# if the meaning is missing, add the description from unit#
#
word[is.na(word$meaning),]$meaning = unit[match(word[is.na(word$meaning),]$pk,unit$pk),]$description#
word[word$meaning=='' & !is.na(word$meaning),]$meaning = NA#
#
# find direct matches through meaning#
word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning.id = word[!is.na(word$meaning.id) & !is.na(word$meaning),]$meaning.id[ match(word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning,word[!is.na(word$meaning.id) & !is.na(word$meaning),]$meaning)]#
sum(is.na(word$meaning.id))#
#
# find direct matches through description#
missing.descriptions = unit[match(word[is.na(word$meaning.id) & !is.na(word$meaning),]$pk,unit$pk),]$description#
wordx = word[!is.na(word$meaning.id) & !is.na(word$meaning),c("pk","meaning.id")]#
word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning.id = sapply(missing.descriptions, function(X){#
	if(is.na(X)){return(NA)}#
	t = table(wordx[wordx$pk %in% unit[unit$description==X,]$pk,]$meaning.id)#
	if(length(t)==0){return(NA)}#
	return(names(t[which(t==max(t))])[1])#
})#
sum(is.na(word$meaning.id))#
#
# TODO!#
missing.meanings = gsub("\\[","",word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning)#
missing.meanings = gsub("\\]","",missing.meanings)#
missing.meanings = gsub("\\*","",missing.meanings)#
missing.meanings = gsub("\\?","",missing.meanings)#
missing.meanings = gsub("\\.","",missing.meanings)#
missing.meanings = gsub("\\)","",missing.meanings)#
missing.meanings = gsub("\\(","",missing.meanings)#
missing.meanings = gsub("\\+","",missing.meanings)#
wordx = word[!is.na(word$meaning.id) & !is.na(word$meaning),c("pk","meaning.id")]#
word[is.na(word$meaning.id) & !is.na(word$meaning),]$meaning.id = sapply(missing.meanings, function(X){#
	if(is.na(X)){return(NA)}	#
	t = table(wordx[wordx$pk %in% unit[#
		grepl(paste("[ ,^;]",X,"[$ ,;]",sep=''),unit$description)#
		,]$pk,]$meaning.id)#
	if(length(t)==0){return(NA)}#
	ans = names(t[which(t==max(t))])[1]#
	#print(c(X,ans))#
	return(ans)#
})#
sum(is.na(word$meaning.id))#
word$POS = meaning[match(word$parameter_pk,meaning$pk),]$semantic_category#
word$semantic_field_pk = meaning[match(word$parameter_pk,meaning$pk),]$semantic_field_pk#
word$semantic_field = semanticfield[match(word$semantic_field_pk,semanticfield$pk),]$name#
#
#word.details = unit[word_to_unit,]$jsondata#
#word$effect = NA#
#word$grammatical_info = NA#
#word$salience = NA#
#word[,c("effect",'grammatical_info','salience')] = sapply(unit[word_to_unit,]$jsondata,function(X){fromJSON(X)[c("effect","grammatical_info","salience")]})#
#
# match word to its original loan.  #
# CAREFUL! there are more than 1:1 mapping from word$pk to loan$target_word!!#
#word$loan = loan[match(word$pk,loan$target_word)]#
#
# Maybe just make seperate table of word sources#
loan$source_word = unit[match(loan$source_word_pk,unit$pk),]$name#
loan$source_language_pk = unit[match(loan$source_word_pk,unit$pk),]$language_pk#
loan$source_language = language[match(loan$source_language_pk,language$pk),]$name#
# use unit_pk in unitvalue.csv (which is just the pk in word.csv) to link to unitdomainelement_pk in unitdomainelement.csv to get year of borrowing.#
#
unitdomainelement$jsondata2 = gsub("[^0-9,\\-]*","",unitdomainelement$jsondata)#
unitdomainelement$start_year = sapply(unitdomainelement$jsondata2,function(X){strsplit(X,",")[[1]][2]})#
unitdomainelement$end_year = sapply(unitdomainelement$jsondata2,function(X){strsplit(X,",")[[1]][1]})#
loan$unitdomainelement_pk = unitvalue[match(loan$target_word_pk,unitvalue$unit_pk),]$unitdomainelement_pk#
loan$start_year = unitdomainelement[match(loan$unitdomainelement_pk,unitdomainelement$pk),]$start_year#
loan$end_year = unitdomainelement[match(loan$unitdomainelement_pk,unitdomainelement$pk),]$end_year#
loan$borrow_year = unitdomainelement[match(loan$unitdomainelement_pk,unitdomainelement$pk),]$name#
write.csv(loan,"../../WOLD_dataextract/loan_processed.csv",row.names=F)#
write.csv(word,"../../WOLD_dataextract/word_processed.csv",row.names=F, fileEncoding='utf-8')
head(word)
celex = read.delim("~/Documents/MPI/CausalGraphs/CELEX_EngWordforms.txt",sep='\\')
celex = read.delim("~/Documents/MPI/CausalGraphs/CELEX_EngWordforms.txt",sep='\\',quote='')
head(celex)
head(AllWOLDdata)
sample(AllWOLDdata$word)
sample(AllWOLDdata$word)
try(setwd("U:/Pragmatics/New/Analysis/"))#
try(setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis"))#
# load data and filter unwanted languages#
# (creates variable 'alldata')#
source("RestrictionsApplied.R") # also loads PermutationTools.R
head(alldata)
unique(unlist(strsplit(alldata$word.simple,'')))
sort(unique(unlist(strsplit(alldata$word.simple,''))))
rm(list=ls())#
#
try(setwd("U:/Pragmatics/Slonimska/Data/RAW_data/WOLD"))#
#
try(setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/RAW_data/WOLD"))#
AllWOLDdata<-read.csv("word_processed.csv",stringsAsFactors=F,encoding='utf-8')#
#
celex = read.delim("~/Documents/MPI/CausalGraphs/CELEX_EngWordforms.txt",sep='\\',quote='')#
#
celex.conv = read.devlim("../../Processing/CharacterSubstitutionsCELEX_converstion.tab")
celex.conv = read.delim("../../Processing/CharacterSubstitutionsCELEX_converstion.tab")
celex.conv = read.delim("../Processing/CharacterSubstitutionsCELEX_converstion.tab")
celex.conv = read.delim("../../Processing/CharacterSubstitutions/CELEX_converstion.tab")
head(celex.conv)
for(i in 1:nrow(celex.conv)){#
	celex$PhonCLX = gsub(celex.conv[i,1],celex.conv[i,2])#
}
celex.conv = read.delim("../../Processing/CharacterSubstitutions/CELEX_converstion.tab")#
for(i in 1:nrow(celex.conv)){#
	celex$PhonCLX = gsub(celex.conv[i,1],celex.conv[i,2],celex$PhonCLX)#
}
