"Kumyk (Kajtak dialect)",
"Kumyk (Kajtak Tumenler dialect)",
"Kumyk (Karabudakhkent dialect)",
"Kumyk (Ter Bragun dialect)",
"Akhvakh (Northern dialect)", # Using southern dialect
"Avar (Kusur dialect)",
"Avar (Andalal dialect)",
"Avar (Antsukh dialect)",
"Avar (Batlukh dialect)",
"Avar (Hid dialect)",
"Avar (Karakh dialect)",
"Avar (Zakataly dialect)",
"Bezhta (Khasharkhota dialect)",
"Bezhta (Tlyadal dialect, Karauzek subdialect)",
"Bulang",
"Bulang-3",
"Botlikh (Miarso dialect)",
"Chechen (Akkin dialect)",
"Chamalal (Gigatli dialect)",
"Dargwa (Tsudakhar dialect, Tanty subdialect)",
"Dargwa (Gapshima dialect)",
"Dargwa (Gapshima Shukti dialect)",
"Dargwa (Gubden dialect)",
"Dargwa (Kadar dialect)",
"Dargwa (Megeb dialect)",
"Dargwa (Mekegi dialect)",
"Dargwa (Mugi dialect)",
"Dargwa (Muiri dialect)",
"Dargwa (Sirkhi dialect)",
"Dargwa (Usisha dialect)",
"Dargwa (Chirag dialect)",
"Dargwa (Itsari dialect)",
"Dargwa (Khajdak dialect)",
"Dargwa (Kubachi dialect)",
"Dargwa (Tsudakhar dialect)",
"Dargwa (Urakhi dialect)",
"Rutul (Mukhrek) dialect",
"Lak (Arakul dialect)",
"Lak (Balkhar dialect)",
"Lak (Shali dialect)",
"Lezgi (Mikrakh dialect)",
"Karata (Tokitin dialect)",
"Khwarshi (Khwarshi dialect)",
"Lezgian (Quba dialect)",
"Mang VN",
"Rutul (Ikhrek dialect)",
"Rutul (Shinaz dialect)",
"Sanapaná (Angaité dialect)",
"Tabasaran (Northern dialect Khanag subdialect)",
"Tsez (Sagada dialect)",
"Azerbaijani (Terekeme dialect)",
"Rutul (Borchino Khnow dialect)",
"Tsakhur (Gelmets dialect)",
"Aghul (Koshan dialect)",
"Andi (Muni dialect)",
"Armenian (Western variety)",
"KNB (a Pearic variety)",
'Proto Polynesian',
'LiHa',
'Tum',
'Kme-2 (Kemie variety)'
)
alldata <- alldata[!alldata$Language %in% deleteLang,]
#######################################################
# Pronouns - in the raw data, there are two types of coding:
#  he/she/it in one entry, and each separately
#  so collapse all he/she/it into one meaning
pronouns = as.numeric(c("2.91","2.92","2.93","2.931","2.932","2.933","2.94","2.941","2.942","2.95","2.96"))
he.she.it = as.numeric(c("2.931","2.932","2.933"))
dx = alldata[alldata$meaning.id.fixed %in% he.she.it & !is.na(alldata$meaning.id.fixed),]
he.she.it.words = tapply(dx$word.simple,dx$Language, paste,collapse=';')
sel = (is.na(alldata$word.simple) | nchar(alldata$word.simple)==0) & !is.na(alldata$meaning.id.fixed) & alldata$meaning.id.fixed==2.93
alldata[sel,]$word.simple = he.she.it.words[alldata[sel,]$Language]
langs.with.hesheit = unique(alldata[!is.na(alldata$meaning.id.fixed) & !is.na(alldata$word.simple) & alldata$meaning.id.fixed==2.93,]$Language)
langs.without.hesheit = setdiff(unique(alldata$Language), langs.with.hesheit)
he.she.it.words = he.she.it.words[langs.without.hesheit]
langs.without.hesheit = langs.without.hesheit[!is.na(he.she.it.words)]
he.she.it.words = he.she.it.words[!is.na(he.she.it.words)]
extraPronounRows = data.frame(
meaning.id=2.93,
meaning='he/she/it',
word = he.she.it.words,
Source = tapply(alldata$Source,alldata$Language,head,n=1)[langs.without.hesheit],
Language = langs.without.hesheit,
iso = tapply(alldata$iso,alldata$Language,head,n=1)[langs.without.hesheit],
glotto = tapply(alldata$glotto,alldata$Language,head,n=1)[langs.without.hesheit],
borrowed_score = NA,
analyzability = NA,
domain = 2,
word.clean = he.she.it.words,
word.simple = he.she.it.words,
meaning.id.fixed = 2.93
)
alldata = rbind(alldata,extraPronounRows)
########################################
# Restrict data by certain criteria
#Restriction 1. Wh-word count benchmark < 4
#split wh-words (their IDs) by language and specify that those that are NA should be treated as O
list.whwords<-tapply(alldata$meaning.id.fixed, alldata$glotto, countWhWords)
#list.whwords[is.na(list.whwords)]<-0
#Remove all languages that have <4 whwords
#make a list of langauges that have <4 whwords
nowhwords<-names(list.whwords)[list.whwords<4]
#exclude nowhwords from alldata
alldata <- alldata[! alldata$glotto %in% nowhwords,]
#Restriction 2. Word count benchmark >= as wh-words mean count.
#Make function that takes only unique meaning.id and shows the number of languages (length)
get_unique_length_meaning<-function(x){
unique_x<-unique(x)
length(unique_x)
}
#create variable that splits data by how many languages have the meaning.id
meaningsbylang<-tapply(alldata$glotto, alldata$meaning.id.fixed,get_unique_length_meaning)
#create variable that splits data by how many languages have each wh-word
whwordsbylang<-tapply(alldata[alldata$meaning.id.fixed %in% whwords,]$glotto, alldata[alldata$meaning.id.fixed %in% whwords,]$meaning.id.fixed,get_unique_length_meaning)
# define threshold (mean proportion of wh-words available) for word count (id meanings) for langauge to be included.
min_crit<-min(whwordsbylang/(length(unique(alldata$glotto))))
# Override with fixed proportion
min_crit = 0.75
prop_meaningsbylang<-meaningsbylang/(length(unique(alldata$glotto)))
randommeanings<-as.numeric(names(prop_meaningsbylang)[prop_meaningsbylang>=min_crit])
#keep only those lines in alldata that are above min-crit (randommeanings)
alldata <- alldata [alldata$meaning.id.fixed %in% randommeanings | alldata$meaning.id.fixed %in% whwords,]
overlap.lang<-tapply(alldata$Language,alldata$glotto,function(X){length(unique(X))})
overlap.glotto <-names(overlap.lang)[overlap.lang>1]
#
# for(glotto.code in overlap.glotto){
# 	lang.names.associated.with.glotto = unique(alldata[alldata$glotto==glotto.code,]$Language)
# 	# select only data with this glotto code
# 	overlapdata <- alldata[alldata$glotto==glotto.code,]
# 	# and only wh words
# 	overlapdata.wh <- overlapdata[overlapdata$meaning.id.fixed %in% whwords,]
#
# 	# get number of concetps (total)
# 	num.of.concepts <- tapply(overlapdata$meaning.id.fixed,overlapdata$Language,get_unique_length_meaning)
#
# 	# get number of wh words
# 	num.of.Wh.words <- tapply(overlapdata.wh$meaning.id.fixed,overlapdata.wh$Language,get_unique_length_meaning)
#
# 	# get entropy of wh words
# 	# convert to special data frame
# 	overlapdata.matrix <- data.frame.to.matrix(overlapdata.wh)
# 	entropy.wh.words <- getWordListEntropy(overlapdata.matrix)
#
# 	#rank langs by max number of wh words, then max entropy, then max number of concepts
# 	rank_langs = order(num.of.Wh.words,entropy.wh.words,num.of.concepts,decreasing=T)
# 	# choose top ranking language
# 	language_to_keep = names(num.of.Wh.words)[rank_langs[1]]
#
# 	# keep only the chosen language
# 	alldata = alldata[alldata$glotto!=glotto.code | alldata$Language==language_to_keep,]
#
#
# }
# Final restriction: Must have more than 400 meanings
numMeaningsPerLang = tapply(alldata$meaning.id.fixed, alldata$glotto, function(X){length(unique(X))})
alldata = alldata[!alldata$glotto %in% names(numMeaningsPerLang[numMeaningsPerLang<400]),]
allx =alldata[,c("Language",'glotto','Source')]
allx = allx[!duplicated(allx),]
write.csv(allx,file = 'LangsInAnalysis.csv', row.names = F, fileEncoding = 'utf-8')
# remove objects we don't need
loaded.functions <- as.vector(lsf.str())
do.not.delete <- c("alldata",'whwords')
rm(list=ls()[! ls() %in% c(loaded.functions,do.not.delete)])
# Add geographical data, langauge family and initial wh-question grammar data.
try(setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Processing"))
l.details = read.csv("../Analysis/LangsInAnalysis.csv", fileEncoding = 'utf-8', encoding = 'utf-8', stringsAsFactors = F)
l.details$area = NA
load("~/Documents/MPI/Neandertals_Collab/fossilSept/pca.per.family/autotyp2015-06-09T09-00.rData")
areas = tapply(as.character(autotyp.geography$area),as.character(autotyp.geography$glottolog_LID.2014),head,n=1)
#l.details$area[nchar(l.details$area)<2] = NA
l.details$area = areas[l.details$glotto]
l.details[l.details$glotto=="sout2746",]$area = 'Southeast Asia'
l.details[l.details$glotto=="enap1235",]$area = 'NE South America'
l.details[l.details$glotto=="sion1247",]$area = 'Andean'
g = read.csv("~/Documents/MPI/Glottolog/glottolog-languoid.csv/languoid.csv",stringsAsFactors=F)
g$fam = g[match(g$family_pk,g$pk),]$name
g$fam[is.na(g$family_pk)] = g$name[is.na(g$family_pk)]
rownames(g) = g$id
l.details$langFam = g[match(l.details$glotto, g$id),]$fam
# Tzotsil is "bookkeeping", real glotto should be tzot1259
l.details[l.details$glotto=='tzot1264',]$langFam = "Mayan"
l.details$latitude = g[l.details$glotto,]$latitude
l.details$longitude = g[l.details$glotto,]$longitude
# wrong lat/long
l.details[l.details$glotto=='rotu1241',c("latitude",'longitude')] = c(-12.5008,177.066)
l.details[l.details$glotto=='hawa1245',]$latitude = 19.6297
l.details[l.details$glotto=='hawa1245',]$longitude = -155.43
#l.details[!is.na(l.details$glotto) & l.details$glotto=='nucl1241',c("longitude","latitude")] = c(115.33,38.87)
geo.fix =matrix(c(
# glotto   #lat  #long
'nucl1241',38.87, 115.33,
"olda1245",42.14,37.11,
"limo1249",18.13, -77.26,
"nuuc1236",47.4623893,-118.3132951,
"gela1265",22.56, 104.70,
"roma1329",49.6558485,18.0328078,
"anga1295",-22.11, -58.91,
"kami1255",25.89, 109.22,
"tokh1242",40.333402, 87.248417,
"tokh1243",39.5708233,71.3739565,
"oldh1241",51.0851933,5.9698196,
"west2376",48.090278, 17.97,
'sout1528',21.92, 44.32,
'east2283', 45, 40,
'west2348', 45, 40,
'sout2745', 14.33,102.99,
'east1436', 27.87,83.39,
'tzot1264', 16.64,-92.74,
'inxo1238', 46.00, 42.27,
'xvar1237', 46.00, 42.27,
'khoc1238', 46.03, 42.12,
'tlya1238', 46.03, 42.12,
'west2394', 12.52, 102.51,
'cuoi1242',19.36, 105.36, # Cuoi
'khap1242',17.91, 105.53,
#'cuoi1242',18.78, 104.77, # LiHa
'mali1278',17.91, 105.53,
'suri1265',14.47, 103.86
#'cuoi1242',18.78, 104.77 # Tum,
),nrow=3)
geo.fix = geo.fix[,geo.fix[1,] %in% l.details$glotto]
for(i in 1:ncol(geo.fix)){
l.details[!is.na(l.details$glotto) & l.details$glotto == geo.fix[1,i],]$latitude = geo.fix[2,i]
l.details[!is.na(l.details$glotto) & l.details$glotto == geo.fix[1,i],]$longitude = geo.fix[3,i]
}
l.details[grepl("Avar ", l.details$Language),c('longitude','latitude')]= list(46.558, 41.7047)
l.details[grepl("Dargwa ", l.details$Language),c('longitude','latitude')]= list(47.4388 , 42.4257)
l.details[grepl("Lezgian ", l.details$Language),c('longitude','latitude')]= list(47.8951,  41.5157)
l.details[grepl("Andi ", l.details$Language),c('longitude','latitude')]= list(46.2919  ,42.8078)
l.details[grepl("Karata ", l.details$Language),c('longitude','latitude')]= list(46.3151 , 42.6501)
l.details[grepl("Tsez ", l.details$Language),c('longitude','latitude')]= list(45.8096,  42.2646)
l.details[grepl("Tabasaran ", l.details$Language),c('longitude','latitude')]= list(47.8379 , 42.0198)
l.details[grepl("Aghul ", l.details$Language),c('longitude','latitude')]= list(47.5843 , 41.9242)
l.details[grepl("Chamalal ", l.details$Language),c('longitude','latitude')]= list(45.995,  42.5024)
l.details[grepl("Bezhta ", l.details$Language),c('longitude','latitude')]= list(45.995,  42.5024)
l.details[grepl("Rutul ", l.details$Language),c('longitude','latitude')]= list(47.3244,  41.6187)
l.details[is.na(l.details$longitude),c("Language",'glotto')]
# Use autotup geo position to find closest area
library(fields)
missing = cbind(as.numeric(l.details[is.na(l.details$area),]$longitude),
as.numeric(l.details[is.na(l.details$area),]$latitude))
autotyp.loc = cbind(autotyp.geography$longitude,
autotyp.geography$latitude)
distx = rdist.earth(missing,autotyp.loc)
closest.match = as.character(autotyp.geography[apply(distx,1,function(X){which(X==min(X,na.rm = T))[1]}),]$area)
l.details[is.na(l.details$area),]$area = closest.match
# Check long differences?
ax = autotyp.geography[autotyp.geography$glottolog_LID.2014 %in% l.details$glotto,c("longitude",'latitude')]
ax.rn = autotyp.geography[autotyp.geography$glottolog_LID.2014 %in% l.details$glotto,c("glottolog_LID.2014")]
lx = l.details[l.details$glotto %in% autotyp.geography$glottolog_LID.2014,c("longitude",'latitude')]
lx.rn = l.details[l.details$glotto %in% autotyp.geography$glottolog_LID.2014,c('glotto')]
lx[,1] = as.numeric(lx[,1])
lx[,2] = as.numeric(lx[,2])
distx2 = rdist.earth(ax[match(lx.rn,ax.rn),],lx)
distx3 = diag(distx2)
lx.rn[which(distx3>1000)]
wals = read.csv("../Analysis/SubjectVerbOrder/wals-language.csv/language.csv", stringsAsFactors = F, fileEncoding = 'utf-8')
wals$glottocode[!is.na(wals$glottocode) & wals$glottocode==''] = NA
l.details$WALS.qpos = wals[match(l.details$glotto, wals$glottocode),]$X93A.Position.of.Interrogative.Phrases.in.Content.Questions
l.details$WALS.qpos[l.details$WALS.qpos==''] = NA
c.grammars = read.csv("../RAW_data/Grammars.csv", stringsAsFactors = F)
c.grammars[c.grammars$Positioning=='',]$Positioning = NA
c.grammars[c.grammars$Possible.Positioning=='',]$Possible.Positioning = NA
c.grammars[is.na(c.grammars$Possible.Positioning),]$Possible.Positioning = c.grammars[is.na(c.grammars$Possible.Positioning),]$Positioning
c.grammars[c.grammars$Glotto=='tzot1259',]$Glotto = 'tzot1264'
c.grammars[c.grammars$Glotto=='gheg1238',]$Glotto = 'tosk1239'
c.grammars[c.grammars$Glotto=='croa1245',]$Glotto = 'sout1528'
c.grammars[!c.grammars$Glotto %in% l.details$glotto,c("X",'IDS',"Glotto")]
l.details$S.qpos = c.grammars[match(l.details$glotto, c.grammars$Glotto),]$Possible.Positioning
ogl = read.table("../RAW_data/grammarCheck/OldGrammarLangs.tab", sep='\t', stringsAsFactors = F,
header = T)
ogl[!ogl$Glottolog.id %in% l.details$glotto,]
l.details$qpos = l.details$WALS.qpos
l.details[is.na(l.details$qpos),]$qpos = l.details[is.na(l.details$qpos),]$S.qpos
l.details = l.details[order(l.details$langFam, l.details$area, l.details$Language),]
write.csv(l.details, file="../Analysis/LangsInAnalysis_withGeoData.csv", fileEncoding = 'utf-8', row.names = F)
l.details[is.na(l.details$area)]
l.details[is.na(l.details$area),]
l.details[is.na(l.details$langFam),]
l.details
table(l.details$qpos)
"German" %in% l.details$Language
try(alldata<-read.csv("../Processing/CleanedAndSimplifiedData/Alldata_simple.csv", stringsAsFactors=F))
"German" %in% alldata$Language
try(setwd("U:/Pragmatics/New/Analysis/"))
try(setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis"))
try(source("PermutationTools.r"))
#make a list of wh-words by taking meaning.id.fixed reference numbers
whwords<-c(17.61,17.62,17.63,17.64,17.65,17.66,17.67,17.68,17.69)
countWhWords <- function(X){
y <- sum(c(17.61,17.62,17.63,17.64,17.65,17.66,17.67,17.68,17.69) %in% X, na.rm=T)
if(is.na(y)){ return(0)}
return(y)
}
#open data file, load all the data
try(alldata<-read.csv("../Processing/CleanedAndSimplifiedData/Alldata_simple.csv", stringsAsFactors=F))
alldata$domain = floor(alldata$meaning.id.fixed)
#Pre-filtering: Disregard those lines that have no entry in word column
alldata <- alldata[nchar(alldata$word)>0,]
# take out lines with no glotto code
alldata <- alldata[!is.na(alldata$glotto),]
# Restriction: No creoles or reconstructed langauges
# Languages must have their own glottocode (removes dialects)
#Take out following langauges:
deleteLang<-c(
# Reconstructed
"Old Chinese",
"Classical Greek",
"Unidentified",
"Classical Arabic",
"Slavic",
"Mongolic",
"Proto Polynesian",
"Ancient Aramaic",
# Creoles
'Jamaican Creole (Limonese Creole dialect)',
"Seychelles Creole",
"Suriname Portuguese",
# Duplicated dialects
'Shan', # we also have another dialect of the same glottocode
"Nung-Fengshan", # We have another version of the dialect
"Kumyk (Dorgeli dialect)", # Using plain "Kumyk" instead of these
"Kumyk (Kajtak dialect)",
"Kumyk (Kajtak Tumenler dialect)",
"Kumyk (Karabudakhkent dialect)",
"Kumyk (Ter Bragun dialect)",
"Akhvakh (Northern dialect)", # Using southern dialect
"Avar (Kusur dialect)",
"Avar (Andalal dialect)",
"Avar (Antsukh dialect)",
"Avar (Batlukh dialect)",
"Avar (Hid dialect)",
"Avar (Karakh dialect)",
"Avar (Zakataly dialect)",
"Bezhta (Khasharkhota dialect)",
"Bezhta (Tlyadal dialect, Karauzek subdialect)",
"Bulang",
"Bulang-3",
"Botlikh (Miarso dialect)",
"Chechen (Akkin dialect)",
"Chamalal (Gigatli dialect)",
"Dargwa (Tsudakhar dialect, Tanty subdialect)",
"Dargwa (Gapshima dialect)",
"Dargwa (Gapshima Shukti dialect)",
"Dargwa (Gubden dialect)",
"Dargwa (Kadar dialect)",
"Dargwa (Megeb dialect)",
"Dargwa (Mekegi dialect)",
"Dargwa (Mugi dialect)",
"Dargwa (Muiri dialect)",
"Dargwa (Sirkhi dialect)",
"Dargwa (Usisha dialect)",
"Dargwa (Chirag dialect)",
"Dargwa (Itsari dialect)",
"Dargwa (Khajdak dialect)",
"Dargwa (Kubachi dialect)",
"Dargwa (Tsudakhar dialect)",
"Dargwa (Urakhi dialect)",
"Rutul (Mukhrek) dialect",
"Lak (Arakul dialect)",
"Lak (Balkhar dialect)",
"Lak (Shali dialect)",
"Lezgi (Mikrakh dialect)",
"Karata (Tokitin dialect)",
"Khwarshi (Khwarshi dialect)",
"Lezgian (Quba dialect)",
"Mang VN",
"Rutul (Ikhrek dialect)",
"Rutul (Shinaz dialect)",
"Sanapaná (Angaité dialect)",
"Tabasaran (Northern dialect Khanag subdialect)",
"Tsez (Sagada dialect)",
"Azerbaijani (Terekeme dialect)",
"Rutul (Borchino Khnow dialect)",
"Tsakhur (Gelmets dialect)",
"Aghul (Koshan dialect)",
"Andi (Muni dialect)",
"Armenian (Western variety)",
"KNB (a Pearic variety)",
'Proto Polynesian',
'LiHa',
'Tum',
'Kme-2 (Kemie variety)'
)
alldata <- alldata[!alldata$Language %in% deleteLang,]
#######################################################
# Pronouns - in the raw data, there are two types of coding:
#  he/she/it in one entry, and each separately
#  so collapse all he/she/it into one meaning
pronouns = as.numeric(c("2.91","2.92","2.93","2.931","2.932","2.933","2.94","2.941","2.942","2.95","2.96"))
he.she.it = as.numeric(c("2.931","2.932","2.933"))
dx = alldata[alldata$meaning.id.fixed %in% he.she.it & !is.na(alldata$meaning.id.fixed),]
he.she.it.words = tapply(dx$word.simple,dx$Language, paste,collapse=';')
sel = (is.na(alldata$word.simple) | nchar(alldata$word.simple)==0) & !is.na(alldata$meaning.id.fixed) & alldata$meaning.id.fixed==2.93
alldata[sel,]$word.simple = he.she.it.words[alldata[sel,]$Language]
langs.with.hesheit = unique(alldata[!is.na(alldata$meaning.id.fixed) & !is.na(alldata$word.simple) & alldata$meaning.id.fixed==2.93,]$Language)
langs.without.hesheit = setdiff(unique(alldata$Language), langs.with.hesheit)
he.she.it.words = he.she.it.words[langs.without.hesheit]
langs.without.hesheit = langs.without.hesheit[!is.na(he.she.it.words)]
he.she.it.words = he.she.it.words[!is.na(he.she.it.words)]
extraPronounRows = data.frame(
meaning.id=2.93,
meaning='he/she/it',
word = he.she.it.words,
Source = tapply(alldata$Source,alldata$Language,head,n=1)[langs.without.hesheit],
Language = langs.without.hesheit,
iso = tapply(alldata$iso,alldata$Language,head,n=1)[langs.without.hesheit],
glotto = tapply(alldata$glotto,alldata$Language,head,n=1)[langs.without.hesheit],
borrowed_score = NA,
analyzability = NA,
domain = 2,
word.clean = he.she.it.words,
word.simple = he.she.it.words,
meaning.id.fixed = 2.93
)
alldata = rbind(alldata,extraPronounRows)
########################################
# Restrict data by certain criteria
#Restriction 1. Wh-word count benchmark < 4
#split wh-words (their IDs) by language and specify that those that are NA should be treated as O
list.whwords<-tapply(alldata$meaning.id.fixed, alldata$glotto, countWhWords)
#list.whwords[is.na(list.whwords)]<-0
#Remove all languages that have <4 whwords
#make a list of langauges that have <4 whwords
nowhwords<-names(list.whwords)[list.whwords<4]
#exclude nowhwords from alldata
alldata <- alldata[! alldata$glotto %in% nowhwords,]
#Restriction 2. Word count benchmark >= as wh-words mean count.
#Make function that takes only unique meaning.id and shows the number of languages (length)
get_unique_length_meaning<-function(x){
unique_x<-unique(x)
length(unique_x)
}
#create variable that splits data by how many languages have the meaning.id
meaningsbylang<-tapply(alldata$glotto, alldata$meaning.id.fixed,get_unique_length_meaning)
#create variable that splits data by how many languages have each wh-word
whwordsbylang<-tapply(alldata[alldata$meaning.id.fixed %in% whwords,]$glotto, alldata[alldata$meaning.id.fixed %in% whwords,]$meaning.id.fixed,get_unique_length_meaning)
# define threshold (mean proportion of wh-words available) for word count (id meanings) for langauge to be included.
min_crit<-min(whwordsbylang/(length(unique(alldata$glotto))))
# Override with fixed proportion
min_crit = 0.75
prop_meaningsbylang<-meaningsbylang/(length(unique(alldata$glotto)))
randommeanings<-as.numeric(names(prop_meaningsbylang)[prop_meaningsbylang>=min_crit])
#keep only those lines in alldata that are above min-crit (randommeanings)
alldata <- alldata [alldata$meaning.id.fixed %in% randommeanings | alldata$meaning.id.fixed %in% whwords,]
overlap.lang<-tapply(alldata$Language,alldata$glotto,function(X){length(unique(X))})
overlap.glotto <-names(overlap.lang)[overlap.lang>1]
#
# for(glotto.code in overlap.glotto){
# 	lang.names.associated.with.glotto = unique(alldata[alldata$glotto==glotto.code,]$Language)
# 	# select only data with this glotto code
# 	overlapdata <- alldata[alldata$glotto==glotto.code,]
# 	# and only wh words
# 	overlapdata.wh <- overlapdata[overlapdata$meaning.id.fixed %in% whwords,]
#
# 	# get number of concetps (total)
# 	num.of.concepts <- tapply(overlapdata$meaning.id.fixed,overlapdata$Language,get_unique_length_meaning)
#
# 	# get number of wh words
# 	num.of.Wh.words <- tapply(overlapdata.wh$meaning.id.fixed,overlapdata.wh$Language,get_unique_length_meaning)
#
# 	# get entropy of wh words
# 	# convert to special data frame
# 	overlapdata.matrix <- data.frame.to.matrix(overlapdata.wh)
# 	entropy.wh.words <- getWordListEntropy(overlapdata.matrix)
#
# 	#rank langs by max number of wh words, then max entropy, then max number of concepts
# 	rank_langs = order(num.of.Wh.words,entropy.wh.words,num.of.concepts,decreasing=T)
# 	# choose top ranking language
# 	language_to_keep = names(num.of.Wh.words)[rank_langs[1]]
#
# 	# keep only the chosen language
# 	alldata = alldata[alldata$glotto!=glotto.code | alldata$Language==language_to_keep,]
#
#
# }
# Final restriction: Must have more than 400 meanings
numMeaningsPerLang = tapply(alldata$meaning.id.fixed, alldata$glotto, function(X){length(unique(X))})
alldata = alldata[!alldata$glotto %in% names(numMeaningsPerLang[numMeaningsPerLang<400]),]
allx =alldata[,c("Language",'glotto','Source')]
allx = allx[!duplicated(allx),]
write.csv(allx,file = 'LangsInAnalysis.csv', row.names = F, fileEncoding = 'utf-8')
# remove objects we don't need
loaded.functions <- as.vector(lsf.str())
do.not.delete <- c("alldata",'whwords')
rm(list=ls()[! ls() %in% c(loaded.functions,do.not.delete)])
try(setwd("U:/Pragmatics/New/Analysis/"))
try(setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis"))
# load data and filter unwanted languages
# (creates variable 'alldata')
source("RestrictionsApplied.R") # also loads PermutationTools.R
source("grammars.R")
source("makeDataVariables.R")
dim(d.wh.m)
areas.d.wh
table(areas.d.wh)
plot(l.details$latitude,l.details$longitude)
plot(l.details$latitude~l.details$longitude)
table(families.d.wh)
length(table(families.d.wh))
table(l.details$Source)
table(l.details$Source,l.details$qpos)
d.wh.m[1:10,1:10]
d.wh.m[1:9,1:10]
getWordListEntropy(d.wh.m,T)
z = getWordListEntropy(d.wh.m,T)
hist(z)
sort(apply(d.wh.m,2,function(X){sum(is.na(X)| nchar(X)==0)}))
sum(is.na(areas.d.wh))
dim(d.wh.possible.initial.m)
dim(d.wh.possible.non.initial.m)
