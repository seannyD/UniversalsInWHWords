paste( "pdfcrop --margins '",
-l*bp2mm, " ",
-t*bp2mm," ",
0, " ",
0, "' --clip EvoLang11_test.pdf EvoLang11_crop.pdf",
sep = '')
paste( "pdfcrop --margins '",
-l*bp2mm, " ",
-t*bp2mm," ",
0, " ",
0, "' --clip --bbox '0 0 0 0' EvoLang11_test.pdf EvoLang11_crop.pdf",
sep = '')
w0 = 210
h0 = 297.1
w = 152.4
h = 228.6
l = (w0 - w)/2
t = (h0 - h)/2
w0 - (2*l)
bp2mm = 2.83467
paste( "pdfcrop --margins '",
-l*bp2mm, " ",
-t*bp2mm," ",
0, " ",
0, "' --clip --bbox '",
0,0,w0*bp2mm,h0*bp2mm,
"' EvoLang11_test.pdf EvoLang11_crop.pdf",
sep = '')
paste( "pdfcrop --margins '",
-l*bp2mm, " ",
-t*bp2mm," ",
0, " ",
0, "' --clip --bbox '",
0," ",0," ",w0*bp2mm," ",h0*bp2mm,
"' EvoLang11_test.pdf EvoLang11_crop.pdf",
sep = '')
paste( "pdfcrop --margins '",
-l*bp2mm, " ",
-t*bp2mm," ",
-l*bp2mm, " ",
-t*bp2mm, "' --clip --bbox '",
0," ",0," ",w0*bp2mm," ",h0*bp2mm,
"' EvoLang11_test.pdf EvoLang11_crop.pdf",
sep = '')
w0 = 210
h0 = 297.1
w = 152.35
h = 228.55
l = (w0 - w)/2
t = (h0 - h)/2
w0 - (2*l)
bp2mm = 2.83467
paste( "pdfcrop --margins '",
-l*bp2mm, " ",
-t*bp2mm," ",
-l*bp2mm, " ",
-t*bp2mm, "' --clip --bbox '",
0," ",0," ",w0*bp2mm," ",h0*bp2mm,
"' EvoLang11_test.pdf EvoLang11_crop.pdf",
sep = '')
library(ape)
rtree(10)
t = rtree(10)
d = matrix(rnorm(100),nrow=10)
t
plot(t)
d
d[upper.tri(d)] = d[lower.tri(d)]
diag(d) = 0
library(ecodist)
??patristic
vcov(t)
?vcov
source("http://www.bioconductor.org/biocLite.R")
biocLite("msa")
stringdist("hound","hand", method = 'osa')
library(stringdist)
stringdist("hound","hand", method = 'osa')
?"stringdist"
12/28
library(ngramr)
install.packages('ngramr')
library(ngramr)
install.packages("~/Desktop/ngramr_1.4.5.gz", repos = NULL)
install.packages("~/Desktop/ngramr_1.4.5.tgz", repos = NULL)
install.packages("~/Desktop/ngramr_1.4.5.tar.gz", repos = NULL)
library(ngramr)
?ngram
x = list()
x[[1]] = list(1, 2, 3)
x[[2]] = list(4, 5, 6)
x[[3]] = list(7, 8, 9)
y = do.call(rbind, x)
y
y = matrix(1:9,nrow=3)
y[1,]
y
x = list()
x[[1]] = list(1, 2, 3)
x[[2]] = list(4, 5, 6)
x[[3]] = list(7, 8, 9)
y = do.call(rbind, x)
y
y[1,]
typeof(y)
y = as.matrix(y)
y
y[1,]
unlist(y)
?do.call
z = matrix(1:9,nrow=3)
typeof(z)
y = do.call(function(X){rbind(unlist(X))}, x)
?do.call
rbindunlist = function(X){rbind(unlist(X))}
do.call(rbindunlist, x)
x
unlist(x)
lapply(x, unlist)
do.call(rbind,lapply(x, unlist))
typeof(do.call(rbind,lapply(x, unlist)))
y = do.call(rbind, lapply(x, unlist))
y[1,]
library(lme4)
cite(lme4)
cite("lme4")
?lme4
?cite
d = read.table("~/Desktop/jaeger.tab", stringsAsFactors = F, header=T,sep='\t')
head(d)
names(d)
image(d[17:40,17:40])
17:40
d[17:40,17:40]
image(as.matrix(d[17:40,17:40]))
heatmap(as.matrix(d[17:40,17:40]))
rownames(d) = colnames(d)
image(as.matrix(d[17:40,17:40]))
heatmap(as.matrix(d[17:40,17:40]))
d = read.table("~/Desktop/jaeger.tab", stringsAsFactors = F, header=T,sep='\t')
rownames(d) = colnames(d)
image(as.matrix(d[17:40,17:40]))
heatmap(as.matrix(d[17:40,17:40]))
try(setwd("U:/Pragmatics/New/Analysis/"))
try(setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis"))
# load data and filter unwanted languages
# (creates variable 'alldata')
source("RestrictionsApplied.R") # also loads PermutationTools.R
source("grammars.R")
source("makeDataVariables.R")
alldata.wh = alldata[alldata$meaning.id.fixed %in% whwords,]
d.wh.noDuplicates = by(alldata.wh[,c("word.clean",'word.simple')], alldata.wh$Language, function(X){
clean = unlist(strsplit(X[,1],";"))
simple = unlist(strsplit(X[,2],";"))
paste(simple[which(!duplicated(clean))],collapse=";")
})
d.wh.noDuplicates = t(as.matrix(d.wh.noDuplicates))
#########
number.of.perms = 10000
number.of.random.samples = 100# number of sets of random concept sets chosen in the comparison permutation for non-wh concepts.
#Specifes where Text file with results goes
resultsFile = "../Results/SimplifiedPhonology/ResultsSummary_noDuplicates.txt"
#Removes old info
cat("",file=resultsFile) # clear results file
mean(getWordListEntropy(d.wh.m, firstSegment=F))
mean(getWordListEntropy(d.wh.m, firstSegment=T))
mean(getWordListEntropy(d.wh.noDuplicates, firstSegment=F))
mean(getWordListEntropy(d.wh.noDuplicates, firstSegment=T))
set.seed(9999)
# XXX TODO These don't make sense because there's only one row
#runPermutation(d.wh.noDuplicates,"AllLangs_allSegments_noDuplicates",F)
#runPermutation(d.wh.noDuplicates,"AllLangs_firstSegments_noDuplicates",T)
dim(d.random.m)
runComparison.wh.random.permutation(d.wh.noDuplicates,d.random.m,"RandomConcepts/Comparison_WH_Random_allSegments_noDuplicates",F)
initial.possible.glotto
colnames(d.wh.noDuplicates)
d.wh.possible.non.initial
names2glotto[colnames(d.wh.possible.initial.m)]
colnames(d.wh.possible.initial.m)
d.wh.noDuplicates[,colnames(d.wh.possible.initial.m)]
d.wh.noDuplicates.initial = d.wh.noDuplicates[,colnames(d.wh.possible.initial.m)]
d.wh.noDuplicates.non.initial = d.wh.noDuplicates[,colnames(d.wh.possible.non.initial.m)]
dim(d.wh.noDuplicates.initial)
d.wh.noDuplicates.initial
dim(d.wh.noDuplicates)
d.wh.noDuplicates.initial = as.matrix(d.wh.noDuplicates[,colnames(d.wh.possible.initial.m)], nrow=1)
d.wh.noDuplicates.initial
dim(d.wh.noDuplicates.initial)
d.wh.noDuplicates.initial = matrix(d.wh.noDuplicates[,colnames(d.wh.possible.initial.m)], nrow=1)
dim(d.wh.noDuplicates.initial)
(d.wh.noDuplicates.initial)
families.d.wh.possible.initial
length(families.d.wh.possible.initial)
dim(d.wh.noDuplicates.initial)
dim(d.wh.noDuplicates.non.initial)
dim(d.wh.non.initial)
dim(d.wh.possible.initial.m)
d.wh.noDuplicates.non.initial = matrix(d.wh.noDuplicates[,colnames(d.wh.possible.non.initial.m)], nrow=1)
runComparison.randomSample(d.wh.noDuplicates.initial,d.wh.noDuplicates.non.initial,families.d.wh.possible.initial,families.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_allSegments",F)
dim(d.wh.noDuplicates.non.initial)
dim(families.d.wh.possible.non.initial)
runComparison.randomSample(d.wh.noDuplicates.initial,d.wh.noDuplicates.non.initial,families.d.wh.possible.initial,families.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_allSegments_noDuplicates",F)
ncol(d.wh.noDuplicates.initial)
ncol(d.wh.noDuplicates.non.initial)
selectRandomIndependentSample(d.wh.noDuplicates.initial, families.d.wh.possible.initial)
wordList = d.wh.noDuplicates.initial
strat = families.d.wh.possible.initial
limit=ncol(wordList)
x = wordList[,tapply(1:ncol(wordList),strat,sample,size=1)]
x
dim(x)
length(x)
typeof(x)
dim(x)
is.null(dim(x))
x = matrix(x)
x
x = matrix(x, nrow=1)
x
x[,sample(1:ncol(x),size=limit)]
limit
x = wordList[,tapply(1:ncol(wordList),strat,sample,size=1)]
length(x)
dim(wordList)
x[,sample(1:ncol(x),size=limit)]
limit = length(strat)
x[,sample(1:ncol(x),size=limit)]
x
x = matrix(x, nrow=1)
limit = length(strat)
limit = length(strat)
x[,sample(1:ncol(x),size=limit)]
limit
limit = length(unique(strat))
x[,sample(1:ncol(x),size=limit)]
selectRandomIndependentSample = function(wordList,strat,limit=ncol(wordList)){
x = wordList[,tapply(1:ncol(wordList),strat,sample,size=1)]
if(is.null(dim(x))){
x = matrix(x, nrow=1)
}
# out of this list, return 'limit' x languages
return(x[,sample(1:ncol(x),size=limit)])
}
runComparison.randomSample(d.wh.noDuplicates.initial,d.wh.noDuplicates.non.initial,families.d.wh.possible.initial,families.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_allSegments_noDuplicates",F)
d.wh.m[1:3,1:3]
d.wh.m[,1:3]
d.wh.m[,1:10]
d.wh.noDuplicates = by(alldata.wh[,c("word.clean",'word.simple')], alldata.wh$Language, function(X){
clean = unlist(strsplit(X[,1],";"))
simple = unlist(strsplit(X[,2],";"))
paste(simple[which(!duplicated(clean))],collapse=";")
})
length(d.wh.noDuplicates)
d.wh.noDuplicates = rbind(
t(as.matrix(d.wh.noDuplicates)),
rep(NA, length(d.wh.noDuplicates))
)
dim(d.wh.noDuplicates)
d.wh.noDuplicates.initial = d.wh.noDuplicates[,colnames(d.wh.possible.initial.m)]
dim(d.wh.noDuplicates)
dim(d.wh.noDuplicates.initial)
d.wh.noDuplicates.non.initial = d.wh.noDuplicates[,colnames(d.wh.possible.non.initial.m)]
runComparison.randomSample(d.wh.noDuplicates.initial,d.wh.noDuplicates.non.initial,families.d.wh.possible.initial,families.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_allSegments_noDuplicates",F)
selectRandomIndependentSample = function(wordList,strat,limit=ncol(wordList)){
x = wordList[,tapply(1:ncol(wordList),strat,sample,size=1)]
#if(is.null(dim(x))){
#  x = matrix(x, nrow=1)
#}
# out of this list, return 'limit' x languages
return(x[,sample(1:ncol(x),size=limit)])
}
runComparison.randomSample(d.wh.noDuplicates.initial,d.wh.noDuplicates.non.initial,families.d.wh.possible.initial,families.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_allSegments_noDuplicates",F)
library(plyr)
library(ggplot2)
library(lattice)
setwd("~/Documents/MPI/EmergingLanguages/SignSpeechConnieBill/Model/")
# 500 stages = 70 years
timescale = 70/500
makePDF = function(pdfName,widthx){
pdf(paste("analysis/graphs/",pdfName,sep=''),width=widthx,height=4)
}
fixLabels = function(dx,labs){
current_labs = unique(dx$runName)
for(i in 1:length(current_labs)){
dx[dx$runName==current_labs[i],]$runName = labs[i]
}
dx$runName = factor(dx$runName,levels = labs)
#dx$runName = relevel(dx$runName,labs[i])
return(dx)
}
plotMeanRun = function(dx, labels, plotIndividualRuns=F,pdfName=NA,plotYears = T,widthx=4,ylimx=c(0,10.5)){
if(!is.na(pdfName)){
makePDF(pdfName,widthx)
}
dx = fixLabels(dx,labels)
if(plotYears){
px = ggplot(data=dx, aes(x=year, y=prop.deaf, group=runName))
} else{
px = ggplot(data=dx, aes(x=stage, y=prop.deaf, group=runName))
}
px = px + geom_smooth(aes(colour=runName), lwd=2)  +
xlab("Years") +
ylab("% Deaf Individuals") +
#	  coord_cartesian(ylim=ylimx) +
theme(legend.position="top", axis.title.y=element_text(angle=90),legend.title=element_blank())
if(plotIndividualRuns){
px = px + 	geom_line(aes(group=runNameNum, colour=runName), alpha=0.2)
}
print(px)
if(!is.na(pdfName)){
dev.off()
}
return(px)
}
plotMeanHearingSigns = function(dx,labels, plotIndividualRuns=F,pdfName=NA,plotYears = T,widthx=4,ylimx=c(0,40)){
if(!is.na(pdfName)){
makePDF(pdfName,widthx)
}
dx = fixLabels(dx,labels)
if(plotYears){
px = ggplot(data=dx, aes(x=year, y=prop.signs,group=runName))
} else{
px = ggplot(data=dx, aes(x=stage, y=prop.signs,group=runName))
}
px = px +geom_smooth(aes(colour=runName), lwd=2) +
xlab("Years") +
ylab("% Sign Vocabulary of Hearing Individuals")+
#	  coord_cartesian(ylim=ylimx) +
theme(legend.position="top", axis.title.y=element_text(angle=90), legend.title=element_blank())
if(plotIndividualRuns){
px = px + geom_line(aes(group=runNameNum, colour=runName), alpha=0.2)
}
print(px)
if(!is.na(pdfName)){
dev.off()
}
return(px)
}
loadData = function(paramName){
res = loadDataHist(paramName)
param = params[params$RunName==paramName,]
popSize = param$nAgents
# 	#popSize = 76
#
# 	folder = paste("results/", paramName,'/',sep='')
# 	res = data.frame()
# 	files = list.files(folder,"*.res")
#
# 	for(i in 1:length(files)){
# 		d = read.csv(paste(folder,files[i],sep=''),header=T,quote='')
# 		names(d) = c("stage","id","deaf","signs","sounds",'structure','age')
# 		d$prop.signs = d$signs/(d$signs+d$sounds)
# 	d$prop.signs[is.nan(d$prop.signs)] = 0
# 		d$runNum = i
# 		res = rbind(res,d)
# 	}
ressum = ddply(res, .(runNum,stage), summarize, prop.signs = mean(prop.signs,na.rm=T), nDeaf = sum(deaf))
ressum$runName = paramName
ressum$runNameNum = paste(ressum$runName,ressum$runNum)
ressum$prop.deaf = ressum$nDeaf / popSize
ressum$prop.deaf = ressum$prop.deaf *100
res$prop.signs = 100* res$prop.signs
ressum$year = ressum$stage * timescale
return(ressum)
}
loadDataHist = function(paramName){
param = params[params$RunName==paramName,]
popSize = param$nAgents
#popSize = 76
folder = paste("results/", paramName,'/',sep='')
res = data.frame()
files = list.files(folder,"*.res")
for(i in 1:length(files)){
d = read.csv(paste(folder,files[i],sep=''),header=T,quote='')
names(d) = c("stage","id","deaf","signs","sounds",'structure','age')
d$prop.signs = d$signs/(d$signs+d$sounds)
d$prop.signs[is.nan(d$prop.signs)] = 0
d$runNum = i
res = rbind(res,d)
}
res$prop.signs = res$signs/(res$signs+res$sounds)
return(res)
}
plotFluency = function(dxx,cuts = c(0.45,0.30,0.1) ,stagex = 100, measure = "prop.signs"){
# Should pass whole population to this function
colx = c("#FF7F00", "#00B0FF")
dx = dxx[dxx$stage==stagex,]
nonfluent = 681/(2189-372)
fluent = 449 / (2189-372)
balanced = 78 / (2189-372)
non = 562 / (2189-372)
nonfluent.chican = 211/(720-156)
fluent.chican = 100 / (720-156)
balanced.chican = 21 / (720-156)
non.chican = 215 / (2189-372)
# balanced
balanced.m = sum(dx[dx$deaf==0,measure] > cuts[1],na.rm=T)/nrow(dx)
# fluent
fluent.m = sum(dx[dx$deaf==0,measure] > cuts[2] & (dx[dx$deaf==0,measure] <= cuts[1]),na.rm=T) / nrow(dx)
# non-fluent
nonfluent.m = sum(dx[dx$deaf==0,measure] > cuts[3] & (dx[dx$deaf==0,measure] <= cuts[2]),na.rm=T)/nrow(dx)
# non-signers
non.m = sum(dx[dx$deaf==0,measure] <= cuts[3],na.rm=T)/nrow(dx)
par(xpd=T)
barplot(rbind(
c(non,nonfluent,fluent,balanced)
# ,c(non.chican, nonfluent.chican,fluent.chican,balanced.chican)
,c(non.m,nonfluent.m,fluent.m,balanced.m)
),beside=T, ylab='Proportion of population',  names.arg = c("Non\nSigner","Non\nFluent",'Fluent','Balanced'), col =colx,border=NA, space=c(0,0.5))
legend(-0.8,0.4,legend=c("Kata Kolok","Model"),col=colx,pch=15,cex=1)
}
##########
params = read.csv("Parameters/KK_Model_parameters.csv",stringsAsFactors=F)
for(i in 1:ncol(params)){
params[is.na(params[,i]),i] = params[1,i]
}
params = rbind(params,params[1,])
params[nrow(params),c("RunName","nAgents")] = c("KK_100Community",100)
params = rbind(params,params[1,])
params[nrow(params),c("RunName","nAgents")] = c("KK_500Community",500)
params = rbind(params,params[1,])
params[nrow(params),c("RunName","nAgents")] = c("KK_800Community",800)
params$nAgents = as.numeric(params$nAgents)
kkDefault = loadData("KK_Default")
test3 = rbind(
loadData("KK_SmallCommunity"),
kkDefault,
loadData("KK_LargeCommunity")
)
test4 = rbind(
loadData("BulelengGeneralSituation"),
kkDefault,
loadData("KK_NoSocialStructure"),
loadData("UrbanSituation")
)
dx = loadData("KK_NoSocialStructure")
head(dx)
max(prop.signs)
max(dx$prop.signs)
table(dx$runNum)
plot(dx$prop.signs~ dx$stage)
kkDefault = loadData("KK_Default")
loadData("KK_NoSocialStructure"),
dx = loadData("KK_NoSocialStructure")
dim(dx)
plot(dx$prop.signs~ dx$stage)
paramName = "KK_NoSocialStructure"
res = loadDataHist(paramName)
param = params[params$RunName==paramName,]
popSize = param$nAgents
ressum = ddply(res, .(runNum,stage), summarize, prop.signs = mean(prop.signs,na.rm=T), nDeaf = sum(deaf))
ressum$runName = paramName
ressum$runNameNum = paste(ressum$runName,ressum$runNum)
ressum
head(ressum)
head(res)
hist(res$prop.signs)
meant(res$prop.signs)
mean(res$prop.signs)
mean(res$prop.signs, na.rm=T)
try(setwd("U:/Pragmatics/New/Analysis/"))
try(setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis"))
# load data and filter unwanted languages
# (creates variable 'alldata')
source("RestrictionsApplied.R") # also loads PermutationTools.R
source("grammars.R")
source("makeDataVariables.R")
alldata.wh = alldata[alldata$meaning.id.fixed %in% whwords,]
# Remove duplicate wh words within languages
d.wh.noDuplicates = by(alldata.wh[,c("word.clean",'word.simple')], alldata.wh$Language, function(X){
clean = unlist(strsplit(X[,1],";"))
simple = unlist(strsplit(X[,2],";"))
paste(simple[which(!duplicated(clean))],collapse=";")
})
d.wh.noDuplicates = rbind(
t(as.matrix(d.wh.noDuplicates)),
rep(NA, length(d.wh.noDuplicates)) # fake row to stop the matrix collapsing
)
d.wh.noDuplicates.initial = d.wh.noDuplicates[,colnames(d.wh.possible.initial.m)]
d.wh.noDuplicates.non.initial = d.wh.noDuplicates[,colnames(d.wh.possible.non.initial.m)]
#########
number.of.perms = 10000
number.of.random.samples = 100# number of sets of random concept sets chosen in the comparison permutation for non-wh concepts.
#Specifes where Text file with results goes
resultsFile = "../Results/SimplifiedPhonology/ResultsSummary_noDuplicates.txt"
#Removes old info
cat("",file=resultsFile) # clear results file
mean(getWordListEntropy(d.wh.m, firstSegment=F))
mean(getWordListEntropy(d.wh.m, firstSegment=T))
mean(getWordListEntropy(d.wh.noDuplicates, firstSegment=F))
mean(getWordListEntropy(d.wh.noDuplicates, firstSegment=T))
set.seed(9999)
resultsFile = "../Results/SimplifiedPhonology/ResultsSummary_RIS_noDuplicates.txt"
runComparison.randomSample(d.wh.noDuplicates.initial,d.wh.noDuplicates.non.initial,families.d.wh.possible.initial,families.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_firstSegments_noDuplicates",T)
number.of.perms = 20000
number.of.random.samples = 100
runComparison.randomSample(d.wh.noDuplicates.initial,d.wh.noDuplicates.non.initial,families.d.wh.possible.initial,families.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_allSegments_noDuplicates",F)
set.seed(327823)
runComparison.randomSample(d.wh.noDuplicates.initial,d.wh.noDuplicates.non.initial,families.d.wh.possible.initial,families.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_allSegments_noDuplicates",F)
runComparison.randomSample(d.wh.noDuplicates.initial,d.wh.noDuplicates.non.initial,families.d.wh.possible.initial,families.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_firstSegments_noDuplicates",T)
set.seed(32)
# Areas
runComparison.randomSample(
d.wh.noDuplicates.initial,
d.wh.noDuplicates.non.initial,
areas.d.wh.possible.initial,
areas.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_allSegments_Areas_noDuplicates",F)
runComparison.randomSample(
d.wh.noDuplicates.initial,
d.wh.noDuplicates.non.initial,
areas.d.wh.possible.initial,
areas.d.wh.possible.non.initial,"InterrogativeOrder/InterrogativeOrder_RandomIndependentSamples_firstSegments_Areas_noDuplicates",T)
