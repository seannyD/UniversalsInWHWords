# t value
(ns_in_nose - mean(propNs)) / sd(propNs)
###
# What words have more Ns than nose?
# use most common meaning to name id
convertMeaningIdToMeanings <- tapply(alldata$meaning, alldata$meaning.id.fixed, function(X){
tail(names(sort(table(X))),1)
})
# words with largest proportion of ns:
lotsOfNs <- propNs[which(propNs>ns_in_nose)]
# set names to text meanings, instead of ids
names(lotsOfNs) <- convertMeaningIdToMeanings[names(lotsOfNs)]
# print list
sort(lotsOfNs)
setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis")
#open data file, load all the data
alldata<-read.csv("../Processing/CleanedAndSimplifiedData/Alldata_simple.csv", stringsAsFactors=F)
# count number of meanings, and choose only concepts with enough data
meaningCounts <- table(alldata$meaning.id.fixed)
enoughData <- names(meaningCounts)[meaningCounts>145]
alldata <- alldata[alldata$meaning.id.fixed %in% enoughData, ]
noseConceptId <- '4.23'
# choose only languages with a nose concept word
langsWithNoseConcept <- unique(alldata[alldata$meaning.id==noseConceptId,]$glotto)
alldata <- alldata[alldata$glotto %in% langsWithNoseConcept,]
# some descriptive stats
# Number of languages
length(unique(alldata$glotto))
# Number of meanings
length(unique(alldata$meaning.id.fixed))
# function for counting number of entries including an 'n'
countNs <- function(X){
ns <- grepl("n",X)
return(sum(ns,na.rm=T)/sum(!is.na(ns)))
}
# number of ns in each concepts
propNs <- tapply(alldata$word.clean, alldata$meaning.id.fixed, countNs)
# number of ns in 'nose' words
ns_in_nose <- propNs[noseConcetpId]
propNs[noseConceptId]
ns_in_nose <- propNs[noseConceptId]
hist(propNs)
abline(v=ns_in_nose, col=2)
sum(propNs > ns_in_nose) / length(propNs)
(ns_in_nose - mean(propNs)) / sd(propNs)
convertMeaningIdToMeanings <- tapply(alldata$meaning, alldata$meaning.id.fixed, function(X){
tail(names(sort(table(X))),1)
})
lotsOfNs <- propNs[which(propNs>ns_in_nose)]
# set names to text meanings, instead of ids
names(lotsOfNs) <- convertMeaningIdToMeanings[names(lotsOfNs)]
# print list
sort(lotsOfNs)
unique(unlist(splitstr(alldata$word.simple,'')))
unique(unlist(strsplit(alldata$word.simple,'')))
setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis")
#open data file, load all the data
alldata<-read.csv("../Processing/CleanedAndSimplifiedData/Alldata_simple.csv", stringsAsFactors=F)
# count number of meanings, and choose only concepts with enough data
meaningCounts <- table(alldata$meaning.id.fixed)
enoughData <- names(meaningCounts)[meaningCounts>145]
alldata <- alldata[alldata$meaning.id.fixed %in% enoughData, ]
noseConceptId <- '4.23'
# choose only languages with a nose concept word
langsWithNoseConcept <- unique(alldata[alldata$meaning.id==noseConceptId,]$glotto)
alldata <- alldata[alldata$glotto %in% langsWithNoseConcept,]
# some descriptive stats
# Number of languages
length(unique(alldata$glotto))
# Number of meanings
length(unique(alldata$meaning.id.fixed))
# function for counting number of entries including an 'n'
countNs <- function(X){
ns <- grepl("[nmÅ‹]",X)
return(sum(ns,na.rm=T)/sum(!is.na(ns)))
}
# number of ns in each concepts
propNs <- tapply(alldata$word.clean, alldata$meaning.id.fixed, countNs)
# number of ns in 'nose' words
ns_in_nose <- propNs[noseConceptId]
# plot the data
hist(propNs)
abline(v=ns_in_nose, col=2)
# proportion of meanings with more Ns than nose
# (p value)
sum(propNs > ns_in_nose) / length(propNs)
# t value
(ns_in_nose - mean(propNs)) / sd(propNs)
convertMeaningIdToMeanings <- tapply(alldata$meaning, alldata$meaning.id.fixed, function(X){
tail(names(sort(table(X))),1)
})
# words with largest proportion of ns:
lotsOfNs <- propNs[which(propNs>ns_in_nose)]
# set names to text meanings, instead of ids
names(lotsOfNs) <- convertMeaningIdToMeanings[names(lotsOfNs)]
# print list
sort(lotsOfNs)
tail(sort(meaningCounts))
names(meaningCounts) = convertMeaningIdToMeanings[names(meaningCounts)]
tail(sort(meaningCounts))
tail(sort(meaningCounts), 10)
tail(sort(meaningCounts), 12)
tail(sort(meaningCounts), 20)
setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis")
#open data file, load all the data
alldata<-read.csv("../Processing/CleanedAndSimplifiedData/Alldata_simple.csv", stringsAsFactors=F)
# count number of meanings, and choose only concepts with enough data
meaningCounts <- table(alldata$meaning.id.fixed)
enoughData <- names(meaningCounts)[meaningCounts>145]
alldata <- alldata[alldata$meaning.id.fixed %in% enoughData, ]
noseConceptId <- '14.11'
hypothesisedSegments <- '[td]'
# choose only languages with a nose concept word
langsWithNoseConcept <- unique(alldata[alldata$meaning.id==noseConceptId,]$glotto)
alldata <- alldata[alldata$glotto %in% langsWithNoseConcept,]
# some descriptive stats
# Number of languages
length(unique(alldata$glotto))
# Number of meanings
length(unique(alldata$meaning.id.fixed))
# function for counting number of entries including an 'n'
countNs <- function(X){
ns <- grepl(hypothesisedSegments,X)
return(sum(ns,na.rm=T)/sum(!is.na(ns)))
}
# number of ns in each concepts
propNs <- tapply(alldata$word.clean, alldata$meaning.id.fixed, countNs)
# number of ns in 'nose' words
ns_in_nose <- propNs[noseConceptId]
# plot the data
hist(propNs)
abline(v=ns_in_nose, col=2)
# proportion of meanings with more Ns than nose
# (p value)
sum(propNs > ns_in_nose) / length(propNs)
# t value
(ns_in_nose - mean(propNs)) / sd(propNs)
###
# What words have more Ns than nose?
# use most common meaning to name id
convertMeaningIdToMeanings <- tapply(alldata$meaning, alldata$meaning.id.fixed, function(X){
tail(names(sort(table(X))),1)
})
# words with largest proportion of ns:
lotsOfNs <- propNs[which(propNs>ns_in_nose)]
# set names to text meanings, instead of ids
names(lotsOfNs) <- convertMeaningIdToMeanings[names(lotsOfNs)]
# print list
sort(lotsOfNs)
setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis")
#open data file, load all the data
alldata<-read.csv("../Processing/CleanedAndSimplifiedData/Alldata_simple.csv", stringsAsFactors=F)
# count number of meanings, and choose only concepts with enough data
meaningCounts <- table(alldata$meaning.id.fixed)
enoughData <- names(meaningCounts)[meaningCounts>145]
alldata <- alldata[alldata$meaning.id.fixed %in% enoughData, ]
noseConceptId <- '9.26'
hypothesisedSegments <- '[k]'
# choose only languages with a nose concept word
langsWithNoseConcept <- unique(alldata[alldata$meaning.id==noseConceptId,]$glotto)
alldata <- alldata[alldata$glotto %in% langsWithNoseConcept,]
# some descriptive stats
# Number of languages
length(unique(alldata$glotto))
# Number of meanings
length(unique(alldata$meaning.id.fixed))
# function for counting number of entries including an 'n'
countNs <- function(X){
ns <- grepl(hypothesisedSegments,X)
return(sum(ns,na.rm=T)/sum(!is.na(ns)))
}
# number of ns in each concepts
propNs <- tapply(alldata$word.clean, alldata$meaning.id.fixed, countNs)
# number of ns in 'nose' words
ns_in_nose <- propNs[noseConceptId]
# plot the data
hist(propNs)
abline(v=ns_in_nose, col=2)
# proportion of meanings with more Ns than nose
# (p value)
sum(propNs > ns_in_nose) / length(propNs)
# t value
(ns_in_nose - mean(propNs)) / sd(propNs)
###
# What words have more Ns than nose?
# use most common meaning to name id
convertMeaningIdToMeanings <- tapply(alldata$meaning, alldata$meaning.id.fixed, function(X){
tail(names(sort(table(X))),1)
})
# words with largest proportion of ns:
lotsOfNs <- propNs[which(propNs>ns_in_nose)]
# set names to text meanings, instead of ids
names(lotsOfNs) <- convertMeaningIdToMeanings[names(lotsOfNs)]
# print list
sort(lotsOfNs)
unique(unlist(strsplit(alldata$word.simple,'')))
setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis")
#open data file, load all the data
alldata<-read.csv("../Processing/CleanedAndSimplifiedData/Alldata_simple.csv", stringsAsFactors=F)
# count number of meanings, and choose only concepts with enough data
meaningCounts <- table(alldata$meaning.id.fixed)
enoughData <- names(meaningCounts)[meaningCounts>145]
alldata <- alldata[alldata$meaning.id.fixed %in% enoughData, ]
targetConceptId <- '9.26'
hypothesisedSegments <- '[k]'
# choose only languages with a target concept word
langsWithtargetConcept <- unique(alldata[alldata$meaning.id==targetConceptId,]$glotto)
alldata <- alldata[alldata$glotto %in% langsWithtargetConcept,]
# some descriptive stats
# Number of languages
length(unique(alldata$glotto))
# Number of meanings
length(unique(alldata$meaning.id.fixed))
# function for counting number of entries including an 'n'
countNs <- function(X){
ns <- grepl(hypothesisedSegments,X)
return(sum(ns,na.rm=T)/sum(!is.na(ns)))
}
# number of ns in each concepts
propNs <- tapply(alldata$word.clean, alldata$meaning.id.fixed, countNs)
# number of ns in 'target' words
ns_in_target <- propNs[targetConceptId]
# plot the data
hist(propNs)
abline(v=ns_in_target, col=2)
# proportion of meanings with more Ns than target
# (p value)
sum(propNs > ns_in_target) / length(propNs)
# t value
(ns_in_target - mean(propNs)) / sd(propNs)
###
# What words have more Ns than target?
# use most common meaning to name id
convertMeaningIdToMeanings <- tapply(alldata$meaning, alldata$meaning.id.fixed, function(X){
tail(names(sort(table(X))),1)
})
# words with largest proportion of ns:
lotsOfNs <- propNs[which(propNs>ns_in_target)]
# set names to text meanings, instead of ids
names(lotsOfNs) <- convertMeaningIdToMeanings[names(lotsOfNs)]
# print list
sort(lotsOfNs)
setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis")
#open data file, load all the data
alldata<-read.csv("../Processing/CleanedAndSimplifiedData/Alldata_simple.csv", stringsAsFactors=F)
# count number of meanings, and choose only concepts with enough data
meaningCounts <- table(alldata$meaning.id.fixed)
enoughData <- names(meaningCounts)[meaningCounts>145]
alldata <- alldata[alldata$meaning.id.fixed %in% enoughData, ]
targetConceptId <- '9.22'
hypothesisedSegments <- '[k]'
# choose only languages with a target concept word
langsWithtargetConcept <- unique(alldata[alldata$meaning.id==targetConceptId,]$glotto)
alldata <- alldata[alldata$glotto %in% langsWithtargetConcept,]
# some descriptive stats
# Number of languages
length(unique(alldata$glotto))
# Number of meanings
length(unique(alldata$meaning.id.fixed))
# function for counting number of entries including an 'n'
countNs <- function(X){
ns <- grepl(hypothesisedSegments,X)
return(sum(ns,na.rm=T)/sum(!is.na(ns)))
}
# number of ns in each concepts
propNs <- tapply(alldata$word.clean, alldata$meaning.id.fixed, countNs)
# number of ns in 'target' words
ns_in_target <- propNs[targetConceptId]
# plot the data
hist(propNs)
abline(v=ns_in_target, col=2)
# proportion of meanings with more Ns than target
# (p value)
sum(propNs > ns_in_target) / length(propNs)
# t value
(ns_in_target - mean(propNs)) / sd(propNs)
###
# What words have more Ns than target?
# use most common meaning to name id
convertMeaningIdToMeanings <- tapply(alldata$meaning, alldata$meaning.id.fixed, function(X){
tail(names(sort(table(X))),1)
})
# words with largest proportion of ns:
lotsOfNs <- propNs[which(propNs>ns_in_target)]
# set names to text meanings, instead of ids
names(lotsOfNs) <- convertMeaningIdToMeanings[names(lotsOfNs)]
# print list
sort(lotsOfNs)
setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis")
#open data file, load all the data
alldata<-read.csv("../Processing/CleanedAndSimplifiedData/Alldata_simple.csv", stringsAsFactors=F)
# count number of meanings, and choose only concepts with enough data
meaningCounts <- table(alldata$meaning.id.fixed)
enoughData <- names(meaningCounts)[meaningCounts>145]
alldata <- alldata[alldata$meaning.id.fixed %in% enoughData, ]
targetConceptId <- '9.22'
hypothesisedSegments <- '[kt]'
# choose only languages with a target concept word
langsWithtargetConcept <- unique(alldata[alldata$meaning.id==targetConceptId,]$glotto)
alldata <- alldata[alldata$glotto %in% langsWithtargetConcept,]
# some descriptive stats
# Number of languages
length(unique(alldata$glotto))
# Number of meanings
length(unique(alldata$meaning.id.fixed))
# function for counting number of entries including an 'n'
countNs <- function(X){
ns <- grepl(hypothesisedSegments,X)
return(sum(ns,na.rm=T)/sum(!is.na(ns)))
}
# number of ns in each concepts
propNs <- tapply(alldata$word.clean, alldata$meaning.id.fixed, countNs)
# number of ns in 'target' words
ns_in_target <- propNs[targetConceptId]
# plot the data
hist(propNs)
abline(v=ns_in_target, col=2)
# proportion of meanings with more Ns than target
# (p value)
sum(propNs > ns_in_target) / length(propNs)
# t value
(ns_in_target - mean(propNs)) / sd(propNs)
###
# What words have more Ns than target?
# use most common meaning to name id
convertMeaningIdToMeanings <- tapply(alldata$meaning, alldata$meaning.id.fixed, function(X){
tail(names(sort(table(X))),1)
})
# words with largest proportion of ns:
lotsOfNs <- propNs[which(propNs>ns_in_target)]
# set names to text meanings, instead of ids
names(lotsOfNs) <- convertMeaningIdToMeanings[names(lotsOfNs)]
# print list
sort(lotsOfNs)
hist(propNs, main='', xlab="Proportion of words with [k] or [t]")
abline(v=ns_in_target, col=2)
ns_in_target
length(unique(alldata$glotto))
length(unique(alldata$meaning.id.fixed))
ns_in_target
setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis")
#open data file, load all the data
alldata<-read.csv("../Processing/CleanedAndSimplifiedData/Alldata_simple.csv", stringsAsFactors=F)
# count number of meanings, and choose only concepts with enough data
meaningCounts <- table(alldata$meaning.id.fixed)
enoughData <- names(meaningCounts)[meaningCounts>145]
alldata <- alldata[alldata$meaning.id.fixed %in% enoughData, ]
targetConceptId <- '9.22'
hypothesisedSegments <- '[kt]'
# choose only languages with a target concept word
langsWithtargetConcept <- unique(alldata[alldata$meaning.id==targetConceptId,]$glotto)
alldata <- alldata[alldata$glotto %in% langsWithtargetConcept,]
# some descriptive stats
# Number of languages
length(unique(alldata$glotto))
# Number of meanings
length(unique(alldata$meaning.id.fixed))
# function for counting number of entries including an 'n'
countNs <- function(X){
ns <- grepl(hypothesisedSegments,X)
return(sum(ns,na.rm=T)/sum(!is.na(ns)))
}
# number of ns in each concepts
propNs <- tapply(alldata$word.clean, alldata$meaning.id.fixed, countNs)
# number of ns in 'target' words
ns_in_target <- propNs[targetConceptId]
# plot the data
hist(propNs, main='', xlab="Proportion of words with [k] or [t]")
abline(v=ns_in_target, col=2)
# proportion of meanings with more Ns than target
# (p value)
sum(propNs > ns_in_target) / length(propNs)
# t value
(ns_in_target - mean(propNs)) / sd(propNs)
###
# What words have more Ns than target?
# use most common meaning to name id
convertMeaningIdToMeanings <- tapply(alldata$meaning, alldata$meaning.id.fixed, function(X){
tail(names(sort(table(X))),1)
})
# words with largest proportion of ns:
lotsOfNs <- propNs[which(propNs>ns_in_target)]
# set names to text meanings, instead of ids
names(lotsOfNs) <- convertMeaningIdToMeanings[names(lotsOfNs)]
# print list
sort(lotsOfNs)
1 - 0.001530222
ccodes = ('EU'="Europe",'NA'="North America",'SA'='South America','OC'="Oceania",'AS'="Asia",'AF'="Africa",'AN'="Antarctica")
ccodes = c('EU'="Europe",'NA'="North America",'SA'='South America','OC'="Oceania",'AS'="Asia",'AF'="Africa",'AN'="Antarctica")
ccodes
rm(list=ls())
setwd("~/Desktop/Stuff/GreatLanguageGame/NewData/pickledDictionaryFiles/continentFiles/")
ccodes = c('EU'="Europe",'NA'="North America",'SA'='South America','OC'="Oceania",'AS'="Asia",'AF'="Africa")
dx = read.delim("NA_tabProb.tab", stringsAsFactors = F)
langs = sort(unique(c(dx$Target,dx$Guess)))
confusionMatrices = list()
for(cn in names(ccodes)){
dx = read.delim(paste(cn,"_tabProb.tab",sep=''), stringsAsFactors = F)
conf = sapply(1:length(langs), function(i){
dxx = dx[dx$Target==langs[i],]
rownames(dxx) = dxx$Guess
return(dxx[langs,]$N)
})
#conf[is.na(conf)] = 0
cname = countryCodes[cn]
confusionMatrices[[cname]] = conf
}
setwd("~/Desktop/Stuff/GreatLanguageGame/NewData/pickledDictionaryFiles/continentFiles/")
ccodes = c('EU'="Europe",'NA'="North America",'SA'='South America','OC'="Oceania",'AS'="Asia",'AF'="Africa")
dx = read.delim("NA_tabProb.tab", stringsAsFactors = F)
langs = sort(unique(c(dx$Target,dx$Guess)))
confusionMatrices = list()
for(cn in names(ccodes)){
dx = read.delim(paste(cn,"_tabProb.tab",sep=''), stringsAsFactors = F)
conf = sapply(1:length(langs), function(i){
dxx = dx[dx$Target==langs[i],]
rownames(dxx) = dxx$Guess
return(dxx[langs,]$N)
})
#conf[is.na(conf)] = 0
cname = countryCodes[cn]
confusionMatrices[[cname]] = conf
}
setwd("~/Desktop/Stuff/GreatLanguageGame/NewData/pickledDictionaryFiles/continentFiles/")
ccodes = c('EU'="Europe",'NA'="North America",'SA'='South America','OC'="Oceania",'AS'="Asia",'AF'="Africa")
dx = read.delim("NA_tabProb.tab", stringsAsFactors = F)
langs = sort(unique(c(dx$Target,dx$Guess)))
confusionMatrices = list()
for(cn in names(ccodes)){
dx = read.delim(paste(cn,"_tabProb.tab",sep=''), stringsAsFactors = F)
conf = sapply(1:length(langs), function(i){
dxx = dx[dx$Target==langs[i],]
rownames(dxx) = dxx$Guess
return(dxx[langs,]$N)
})
#conf[is.na(conf)] = 0
cname = ccodes[cn]
confusionMatrices[[cname]] = conf
}
dists = matrix(NA,nrow=nrow(langCounts),ncol=nrow(langCounts))
dists = matrix(NA,nrow=length(ccodes),ncol=length(ccodes))
rownames(dists) = ccodes
colnames(dists) = ccodes
for(i in 1:(length(confusionMatrices)-1)){
for(j in (i+1):length(confusionMatrices)){
from = names(confusionMatrices)[i]
to = names(confusionMatrices)[j]
cdist = mean(abs(confusionMatrices[[i]] - confusionMatrices[[j]]),na.rm=T)
dists[from,to] = cdist
}
}
dists[lower.tri(dists)] = dists[upper.tri(dists)]
diag(dists) = 0
write.csv(dists,file="DistanceBetweenContinents.csv")
library(mcclust)
hc = hclust(as.dist(dists))
plot(hc)
header = paste("#nexus\n\nBEGIN Taxa;\nDIMENSIONS ntax=",nrow(dists),";\nTAXLABELS\n",collapse="")
taxlabels= paste(paste("[",1:nrow(dists),"] '",rownames(dists),"'",sep=''),collapse='\n')
header2 = paste("\n;\nEND;  [TAXA]\n\nBEGIN DISTANCES;\n        DIMENSIONS NTAX=" , nrow(dists),";  FORMAT  TRIANGLE=BOTH DIAGONAL LABELS=LEFT;\nMATRIX\n", collapse='')
rnames = paste("'",rownames(dists),"'",sep='')
mat = paste(paste(rnames,apply(dists,1,paste,collapse=' ')),collapse='\n')
header3 = "\n;\nEND;\n"
nexus = paste(header, taxlabels, header2, mat, header3, collapse='')
cat(nexus,file = "nexusFiles/continentDistances.nex")
confusionMatrices[[1]]
setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis")
#open data file, load all the data
alldata<-read.csv("../Processing/CleanedAndSimplifiedData/Alldata_simple.csv", stringsAsFactors=F)
# count number of meanings, and choose only concepts with enough data
meaningCounts <- table(alldata$meaning.id.fixed)
enoughData <- names(meaningCounts)[meaningCounts>145]
alldata <- alldata[alldata$meaning.id.fixed %in% enoughData, ]
targetConceptId <- '9.22'
hypothesisedSegments <- '[kt]'
# choose only languages with a target concept word
langsWithtargetConcept <- unique(alldata[alldata$meaning.id==targetConceptId,]$glotto)
alldata <- alldata[alldata$glotto %in% langsWithtargetConcept,]
# some descriptive stats
# Number of languages
length(unique(alldata$glotto))
# Number of meanings
length(unique(alldata$meaning.id.fixed))
# function for counting number of entries including an 'n'
countNs <- function(X, strx=" "){
ns <- grepl(strx,X)
return(sum(ns,na.rm=T)/sum(!is.na(ns)))
}
# number of ns in each concepts
propNs <- tapply(alldata$word.clean, alldata$meaning.id.fixed, countNs, strx=hypothesisedSegments)
# number of ns in 'target' words
ns_in_target <- propNs[targetConceptId]
# plot the data
hist(propNs, main='', xlab="Proportion of words with [k] or [t]")
abline(v=ns_in_target, col=2)
# proportion of meanings with more Ns than target
# (p value)
sum(propNs > ns_in_target) / length(propNs)
# z value
(ns_in_target - mean(propNs)) / sd(propNs)
###
# What words have more Ns than target?
# use most common meaning to name id
convertMeaningIdToMeanings <- tapply(alldata$meaning, alldata$meaning.id.fixed, function(X){
tail(names(sort(table(X))),1)
})
# words with largest proportion of ns:
lotsOfNs <- propNs[which(propNs>ns_in_target)]
# set names to text meanings, instead of ids
names(lotsOfNs) <- convertMeaningIdToMeanings[names(lotsOfNs)]
# print list
sort(lotsOfNs)
