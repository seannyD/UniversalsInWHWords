head(matchedNPerm.means)
wh.Mean.All
getStats(matchedNPerm.means[1,],wh.Mean.All)
permE= matchedNPerm.means[1,]
permE
table(permE)
length(unique((permE))
)
wh.Mean.All
p =sum(permE <= realE, na.rm = T)
realE = wh.Mean.All
p =sum(permE <= realE, na.rm = T)
p
(realE - mean(permE,na.rm=T)) / sd(permE,na.rm=T)
mean(permE,na.rm=T)
head(permE)
permE[1]
permE[1]+1
mean(permE,na.rm=TRUE)
max(permE)
min(permE)
mean(permE)
mean(as.vector(permE))
mean(as.vector(permE),na.rm=T)
typeof(permE)
unlist(permE)
getStats(unlist(matchedNPerm.means[1,]),wh.Mean.All)
getStats(unlist(matchedNPerm.means[2,]),wh.Mean.First)
getStats(unlist(matchedNPerm.means[4,]),wh.Mean.First)
wh.Mean.First
getStats(unlist(matchedNPerm.means[4,]),wh.Mean.First)
getStats(unlist(matchedNPerm.means[2,]),wh.Mean.First)
getStats(unlist(matchedNPerm.means[1,]),wh.Mean.All)
getStats(unlist(matchedNPerm.means[3,]),wh.Mean.All)
apply(d.wh.m, 2, function(X){x = unlist(strsplit(X,';'))
sum(nchar(x)>0,na.rm=T)})
table()
table(apply(d.wh.m, 2, function(X){x = unlist(strsplit(X,';'))
sum(nchar(x)>0,na.rm=T)}))
dim(d.wh.m)
z = apply(d.wh.m, 2, function(X){x = unlist(strsplit(X,';'))
sum(nchar(x)>0,na.rm=T)})
mean(z)
median(z)
table(z)
median(z)
mode(z)
sd(z)
mean(z)
getStats(unlist(matchedNPerm.means[2,]),wh.Mean.First)
getStats(unlist(matchedNPerm.means[2,]),wh.Mean.First)
getStats(unlist(matchedNPerm.means[4,]),wh.Mean.First)
getStats(unlist(matchedNPerm.means[2,]),wh.Mean.First)
getStats(unlist(matchedNPerm.means[1,]),wh.Mean.All)
getStats(unlist(matchedNPerm.means[4,]),wh.Mean.First)
getStats(unlist(matchedNPerm.means[3,]),wh.Mean.All)
mean(unlist(matchedNPerm.means[1,]))
sum(nchar(x)>0,na.rm=T)})
z2 = apply(d.PronounConcepts.m, 2, function(X){x = unlist(strsplit(X,';'))
sum(nchar(x)>0,na.rm=T)})
mean(z2)
median(z2)
numOfPronounPerLang = apply(d.PronounConcepts.m, 2, function(X){x = unlist(strsplit(X,';'))
sum(nchar(x)>0,na.rm=T)})
numOfPronounPerLang
numWordsPerLang
length(numWordsPerLang)
length(numOfPronounPerLang)
apply(cbind(numOfPronounPerLang, numWordsPerLang), 2, min)
apply(cbind(numOfPronounPerLang, numWordsPerLang), 1, min)
which(!names(numOfPronounPerLang) %in% names(numWordsPerLang))
names(numOfPronounPerLang)
names(numWordsPerLang)
length(numWordsPerLang)
numOfPronounPerLang = apply(d.PronounConcepts.m,2, function(X){
x = unlist(strsplit(X,";"))
x = x[nchar(x)>0 & !is.na(x)]
length(x)
})
length(numOfPronounPerLang)
length(numWordsPerLang)
!names(numOfPronounPerLang) %in% names(numWordsPerLang))
!names(numOfPronounPerLang) %in% names(numWordsPerLang)
which(!names(numWordsPerLang) %in% names(numOfPronounPerLang))
which(!names(numWordsPerLang) %in% names(numOfPronounPerLang))[numWordsPerLang]
names(numWordsPerLang[which(!names(numWordsPerLang) %in% names(numOfPronounPerLang))]
)
names(numWordsPerLang)[which(!names(numWordsPerLang) %in% names(numOfPronounPerLang))]
numOfPronounPerLang["Tsimshian"] = 0
length(numOfPronounPerLang)
numOfPronounPerLang = numOfPronounPerLang[names(numWordsPerLang)]
apply(cbind(numOfPronounPerLang, numWordsPerLang), 1, min)
numPWords = apply(cbind(numOfPronounPerLang, numWordsPerLang), 1, min)
X = d.wh.m
Y = d.PronounConcepts.m
i = 1
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2
Y2
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
X2
Y2
getWordListEntropy(cbind(X2,Y2))
ret= sapply(1:ncol(X), function(X){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
return(diff(getWordListEntropy(cbind(X2,Y2))))
})
dim(X)
i
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
diff(getWordListEntropy(cbind(X2,Y2)))
ret= sapply(1:ncol(X), function(X){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
return(diff(getWordListEntropy(cbind(X2,Y2))))
})
dim(d.PronounConcepts.m)
dim(d.wh.m)
d.PronounConcepts.m[,"Tsimshian"] = rep(NA,6)
d.PronounConcepts.m2 = d.PronounConcepts.m
d.PronounConcepts.m[,"Tsimshian"] = rep(NA,6)
d.PronounConcepts.m = cbind(d.PronounConcepts.m, rep(NA,6))
d.PronounConcepts.m2 = d.PronounConcepts.m2[,colnames(d.wh.m)]
colnames(d.wh.m)
try(setwd("U:/Pragmatics/New/Analysis/"))
try(setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis"))
# load data and filter unwanted languages
# (creates variable 'alldata')
source("RestrictionsApplied.R") # also loads PermutationTools.R
source("grammars.R")
source("makeDataVariables.R")
d.PronounConcepts.m2 = d.PronounConcepts.m
d.PronounConcepts.m2 = cbind(d.PronounConcepts.m, Tsimshian=rep(NA,6))
d.PronounConcepts.m2 = d.PronounConcepts.m2[,colnames(d.wh.m)]
X = d.PronounConcepts.m2
Y = d.wh.m
ret= sapply(1:ncol(X), function(X){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
return(diff(getWordListEntropy(cbind(X2,Y2))))
})
ret= sapply(1:ncol(X), function(i){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
return(diff(getWordListEntropy(cbind(X2,Y2))))
})
for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
return(diff(getWordListEntropy(cbind(X2,Y2))))
}
for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
diff(getWordListEntropy(cbind(X2,Y2)))
}
i
X[,i]
d.PronounConcepts.m2 = d.PronounConcepts.m
d.PronounConcepts.m2 = cbind(d.PronounConcepts.m, Tsimshian=rep("",6))
d.PronounConcepts.m2 = d.PronounConcepts.m2[,colnames(d.wh.m)]
X = d.PronounConcepts.m2
for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
diff(getWordListEntropy(cbind(X2,Y2)))
}
i
X[,i]
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
length(X2)
ret= sapply(1:ncol(X), function(i){
# for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
if(length(X2)==0){
return(NA)
} else{
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
return(diff(getWordListEntropy(cbind(X2,Y2))))
}
})
ret
hist(ret)
sum(ret < 0)
sum(ret < 0,na.rm=T)
length(ret)
151/225
ret= sapply(1:ncol(X), function(i){
# for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
if(length(X2)==0){
return(NA)
} else{
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
return(diff(getWordListEntropy(cbind(X2,Y2))))
}
})
sum(ret < 0,na.rm=T)
xE = getWordListEntropy(cbind(X2))
X2
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
X2
i = 1
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
X2
xE = getWordListEntropy(cbind(X2))
getWordListEntropy(cbind(X2))
xE = getWordListEntropy(cbind(X2))
yE = getWordListEntropy(cbind(Y2))
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
xE = getWordListEntropy(cbind(X2))
yE = getWordListEntropy(cbind(Y2))
xE - yE
xE
yE
sampleMin(d.PronounConcepts.m2, d.wh.m)
sampleMin = function(X,Y){
ret= sapply(1:ncol(X), function(i){
# for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
if(length(X2)==0){
return(NA)
} else{
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
xE = getWordListEntropy(cbind(X2))
yE = getWordListEntropy(cbind(Y2))
return(xE - yE)
}
})
# XXX TODO
}
sampleMin(d.PronounConcepts.m2, d.wh.m)
sampleMin = function(X,Y){
ret= sapply(1:ncol(X), function(i){
# for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
if(length(X2)==0){
return(NA)
} else{
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
xE = getWordListEntropy(cbind(X2))
yE = getWordListEntropy(cbind(Y2))
return(xE - yE)
}
})
return(ret)
}
sampleMin(d.PronounConcepts.m2, d.wh.m)
ret= sapply(1:ncol(X), function(i){
# for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
if(length(X2)==0){
return(NA)
} else{
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
xE = getWordListEntropy(cbind(X2))
yE = getWordListEntropy(cbind(Y2))
return(c(xE,yE))
}
})
ret
sapply(ret,head,n=1)
sapply(ret,tail,n=1)
sampleMin = function(X,Y){
ret= sapply(1:ncol(X), function(i){
# for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
if(length(X2)==0){
return(NA)
} else{
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
xE = getWordListEntropy(cbind(X2))
yE = getWordListEntropy(cbind(Y2))
return(c(xE,yE))
}
})
xE = mean(sapply(ret,head,n=1),na.rm=T)
yE = mean(sapply(ret,head,n=1),na.rm=T)
return(xE -yE)
}
sampleMin(d.PronounConcepts.m2, d.wh.m)
sampleMin = function(X,Y){
ret= sapply(1:ncol(X), function(i){
# for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
if(length(X2)==0){
return(NA)
} else{
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
xE = getWordListEntropy(cbind(X2))
yE = getWordListEntropy(cbind(Y2))
return(c(xE,yE))
}
})
xE = mean(sapply(ret,head,n=1),na.rm=T)
yE = mean(sapply(ret,tail,n=1),na.rm=T)
return(xE -yE)
}
sampleMin(d.PronounConcepts.m2, d.wh.m)
sampleMin = function(X,Y){
ret= sapply(1:ncol(X), function(i){
# for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
if(length(X2)==0){
return(NA)
} else{
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
xE = getWordListEntropy(cbind(X2))
yE = getWordListEntropy(cbind(Y2))
return(c(xE,yE))
}
})
xE = mean(sapply(ret,head,n=1),na.rm=T)
yE = mean(sapply(ret,tail,n=1),na.rm=T)
return(xE -yE)
}
sampleMin(d.PronounConcepts.m2, d.wh.m)
sampleMin = function(X,Y){
ret= sapply(1:ncol(X), function(i){
# for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
if(length(X2)==0){
return(NA)
} else{
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
xE = getWordListEntropy(cbind(X2))
yE = getWordListEntropy(cbind(Y2))
return(c(xE,yE))
}
})
xE = mean(sapply(ret,head,n=1),na.rm=T)
yE = mean(sapply(ret,tail,n=1),na.rm=T)
return(xE -yE)
}
sampleMin(d.PronounConcepts.m2, d.wh.m)
sampleMin = function(X,Y){
ret= sapply(1:ncol(X), function(i){
# for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
if(length(X2)==0){
return(NA)
} else{
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
xE = getWordListEntropy(cbind(X2))
yE = getWordListEntropy(cbind(Y2))
return(c(xE,yE))
}
})
xE = mean(sapply(ret,head,n=1),na.rm=T)
yE = mean(sapply(ret,tail,n=1),na.rm=T)
return(xE -yE)
}
sampleMin(d.PronounConcepts.m2, d.wh.m)
sampleMin = function(X,Y){
ret= sapply(1:ncol(X), function(i){
# for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
if(length(X2)==0){
return(NA)
} else{
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
xE = getWordListEntropy(cbind(X2))
yE = getWordListEntropy(cbind(Y2))
return(c(xE,yE))
}
})
xE = mean(sapply(ret,head,n=1),na.rm=T)
yE = mean(sapply(ret,tail,n=1),na.rm=T)
# negative number = yE smaller than xE
return(yE - xE)
}
sampleMin(d.PronounConcepts.m2, d.wh.m)
res = replicate(1000,sampleMin(d.PronounConcepts.m2, d.wh.m))
sampleMin = function(X,Y){
ret= sapply(1:ncol(X), function(i){
# for(i in 1:ncol(X)){
X2 = unlist(strsplit(X[,i],";"))
X2= X2[!is.na(X2) & nchar(X2)>0]
if(length(X2)==0){
return(NA)
} else{
Y2 = unlist(strsplit(Y[,i],";"))
Y2= Y2[!is.na(Y2) & nchar(Y2)>0]
X2 = sample(X2,min(length(X2), length(Y2)))
Y2 = sample(Y2,min(length(X2), length(Y2)))
xE = getWordListEntropy(cbind(X2),T)
yE = getWordListEntropy(cbind(Y2),T)
return(c(xE,yE))
}
})
xE = mean(sapply(ret,head,n=1),na.rm=T)
yE = mean(sapply(ret,tail,n=1),na.rm=T)
# negative number = yE smaller than xE
return(yE - xE)
}
# negative numebr = wh smaller than pronouns
res = replicate(1000,sampleMin(d.PronounConcepts.m2, d.wh.m))
res = replicate(10,sampleMin(d.PronounConcepts.m2, d.wh.m))
res
sum(res < 0) / sum(!is.na(res))
sum(res >= 0) / sum(!is.na(res))
(0 - mean(res,na.rm=T)) / sd(res,na.rm=T)
res = replicate(1000,sampleMin(d.PronounConcepts.m2, d.wh.m))
sum(res >= 0) / sum(!is.na(res))
(0 - mean(res,na.rm=T)) / sd(res,na.rm=T)
1/1000
mean(res)
sampleMin
dim(d.wh.possible.initial.m)
dim(d.wh.possible.initial.m) + dim(d.wh.possible.non.initial.m)
try(setwd("U:/Pragmatics/New/Analysis/"))
try(setwd("~/Documents/MPI/SemanticsPragmatics/2015Course/Projects/Slonimska/NewAnalysis/Pragmatics_Slonimska/Analysis"))
# load data and filter unwanted languages
# (creates variable 'alldata')
source("RestrictionsApplied.R") # also loads PermutationTools.R
source("grammars.R")
source("makeDataVariables.R")
alldata = alldata[alldata$glotto %in% names2glotto[colnames(d.wh.m)],]
write.csv(alldata,"../Writeup/SupportingInformation/S2_LexicalData.csv", row.names = F)
write.csv(alldata,"../RAW_data/S2_LexicalData.csv", row.names = F)
