\documentclass[10pt,a4paper,landscape]{article}
\usepackage[margin=0.5in]{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\usepackage{natbib}
%\usepackage{citeref}
\usepackage{graphicx}
\usepackage{color}
\usepackage{url}
\usepackage{pdflscape}
\usepackage[table]{xcolor}
\usepackage{longtable}

\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry
\usepackage{setspace}
\title{Full Permutation Summaries}
\author{}
\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}
\maketitle

\section{Mean language entropy compared to between-language permutation}
\input{SummaryTables.tex}

\clearpage
\newpage
\section{Compare entropy of wh words to other sets of words}

\input{SummaryTables2.tex}

\clearpage
\newpage

\section{Random independent samples tests}

\input{SummaryTables3.tex}

\input{SummaryTables4.tex}

\clearpage
\newpage
\section{Tests without duplicates}

In several languages the same form is listed under more than one question-word concept.  This decreases the entropy score, so it is reasonable to ask whether the results are driven by this effect.  However, this is a difficult criticism to address.  In an extreme reading, our hypothesis would predict that a language would have identical forms for all wh-words.  Therefore, removing the duplicates removes part of the effect we are trying to detect.  

Also, we made the following assumptions about the data: Empty cells indicate that the language has no lexicalised form for the concept.  Duplicated forms mean that they use the same form for both concepts.  So, if the language really only had one concept for \emph{how} and \emph{what}, then it would receive only one entry.  Duplicate entries suggest that speakers have separate concepts, but identical forms.  In this case, we think it is fair to count them as separate entries.

Actually implementing this check is also difficult.  A lot of time was put into cleaning and simplifying the representations of words, often on a language-by-language basis.  Forms that look identical in the final data may actually be phonemically different in their raw form.  It is also not clear, if a language has a duplicate form, which concept to exclude (making study 1 difficult to repeat exactly).

Still, the results below are based on a dataset with duplicates within languages removed (conservatively based on the cleaned forms, see testDuplicated.R).  Duplicate forms for other concepts were not removed.  

When removing duplicates, the mean entropy of wh words first segments was 0.48 (compared to 0.46 with duplicates), and when looking at all segments it was 0.59 (compared to 0.57 with duplicates).  The results below do not differ much from the original results, so we conclude that the effects in the main paper are not driven by an artefact of duplicated forms. 

\input{SummaryTables_noDuplicates.tex}

\clearpage
\newpage

\section{Controlling for number of words in permutation tests}

The permutation tests where interrogative words were compared to random words worked by keeping the number of concepts equal in both sets.  So, for example, the $E_f$ score would be calculated for the entries for 9 interrogative concepts and for 9 random concepts.  One concern here is that this might compare different numbers of words.  For example, the 9 interrogative concepts might contain 10 unique words (due to multiple words in 1 entry), while the 9 random concepts might contain 8 unique words (due to missing data).  Since larger samples are more likely to produce extreme values, it's possible that this was affecting the results.

(We note that often the interrogative set had more words per language than many of the other sets, and in this case a random additional word is more likely to increase the entropy than decrease it, so the prediction would be that controlling for number of concepts is conservative)

We re-ran the test from study 2, but keeping the number of unique words in each language the same.  So, for example, if language X had 8 unique interrogative words, then 8 words were selected from the random set.  However, the languages in the sample had between 5 and 25 unique words for interrogatives (median = 10.8, sd = 3), so it was impossible to keep both the number of concepts and the number of words equal.  So, the procedure worked like this:  For each language $L_i$ from $L_1$ to $L_{226}$, the number of unique interrogative words was obtained, call this $U_{L_i}$.  The data for random concepts was organised as a matrix, with rows representing concepts and columns representing languages.  The order of rows was randomly permuted.  For each column, the first $U_{L_i}$ words in the matrix were taken as the corresponding set of words.  In this way, each language had a set of interrogative words, and an equal number of random words, with most languages being matched on concepts as far as it was possible.

Table \ref{tab:ControlNRes} below shows the results of the tests, comparing the original tests (controlling for number of concepts) to the test controlling for number of words.  The latter results in a more extreme result than the original (more in favour of our hypothesis), for both first segments and all segments.  Thus, we argue that the results in the paper are not driven by comparing different numbers of words.

\begin{table}[htdp]
\begin{center}
\begin{tabular}{llll}
\hline
& Controlling for ... & p & z \\
\hline
First segments & Number of concepts & $< $ 0.001 & -27.93 \\
 & Number of words & $<$  0.001 & -34.13 \\
All segments & Number of concepts & $<$  0.001 & -8.49 \\
 & Number of words & $<$  0.001 & -9.34 \\
\end{tabular}
\end{center}
\label{tab:ControlNRes}
\end{table}%

We also performed an additional test comparing the mean $E_f$ for interrogative words to the mean $E_f$ for pronouns.  In this test, a sample of words from each set was taken for each language, so that the size of the sample was the minimum number of words available.  So, if a language had 9 interrogative words and 6 pronoun words, a random sample of 6 interrogative words was taken from the full set of 9.  Mean $E_f$ was calculated for both sets, and the difference taken. This was repeated 1000 times.  

As in the original experiment, we found that interrogative words were more similar (smaller $E_f$) than pronouns (for first segments: mean difference in $E_f$ = -0.15, p $<$ 0.001, z = 24.67).

\bibliographystyle{apalike}
\bibliography{/Users/sgroberts/Documents/PhD/Biblography}


\end{document}